{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to improve each articles analysis with more features that go beyond text. I created dictionaries to detect industries, job roles, technologies, organizations and to perform my custom sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FinBERT model (you'll need to install: pip install transformers torch)\n",
    "class FinBERTSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model.eval()\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Returns sentiment score between -1 (negative) and 1 (positive)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Truncate text if too long for BERT\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                predictions = softmax(outputs.logits, dim=-1)\n",
    "                \n",
    "            # FinBERT outputs: [negative, neutral, positive]\n",
    "            neg_score = predictions[0][0].item()\n",
    "            neu_score = predictions[0][1].item()  \n",
    "            pos_score = predictions[0][2].item()\n",
    "            \n",
    "            # Convert to -1 to 1 scale\n",
    "            sentiment_score = pos_score - neg_score\n",
    "            return sentiment_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in FinBERT analysis: {e}\")\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732e3d1ee43b4e898e8106f7c747b481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee70eafb1bb45aa99c928fc546239e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287a204d67a64971955aabb8efa9d31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcf5b480ca44548b45daf33ed975e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea04a8118a904a7fb3e2175984de32da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9754edf4cd0c4e58b85a1e3e43d56434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Initialize the FinBERT analyzer globally\n",
    "finbert_analyzer = FinBERTSentimentAnalyzer()\n",
    "\n",
    "def detect_negations(text, target_terms):\n",
    "    \"\"\"\n",
    "    Detect negated sentiment terms and return adjusted scores\n",
    "    \"\"\"\n",
    "    negation_patterns = [\n",
    "        r'\\b(?:not|never|no|nothing|nowhere|neither|nobody|none)\\s+\\w*\\s*',\n",
    "        r'\\b(?:don\\'t|doesn\\'t|didn\\'t|won\\'t|wouldn\\'t|can\\'t|cannot|couldn\\'t|shouldn\\'t)\\s+\\w*\\s*',\n",
    "        r'\\b(?:hardly|barely|scarcely|rarely)\\s+\\w*\\s*'\n",
    "    ]\n",
    "    \n",
    "    negated_terms = []\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in negation_patterns:\n",
    "        matches = re.finditer(pattern, text_lower)\n",
    "        for match in matches:\n",
    "            # Check if any target terms appear within 3 words after negation\n",
    "            start_pos = match.end()\n",
    "            next_words = text_lower[start_pos:start_pos+50]  # Check next ~50 characters\n",
    "            \n",
    "            for term in target_terms:\n",
    "                if term.lower() in next_words:\n",
    "                    negated_terms.append(term)\n",
    "    \n",
    "    return negated_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recency_weight(text, ai_terms):\n",
    "    \"\"\"\n",
    "    Give higher weight to AI terms that appear later in the text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # Find positions of AI terms\n",
    "    ai_positions = []\n",
    "    for term in ai_terms:\n",
    "        pos = text_lower.find(term.lower())\n",
    "        while pos != -1:\n",
    "            ai_positions.append(pos / text_length)  # Normalize position\n",
    "            pos = text_lower.find(term.lower(), pos + 1)\n",
    "    \n",
    "    if not ai_positions:\n",
    "        return 1.0\n",
    "    \n",
    "    # Calculate recency weight (terms appearing later get higher weight)\n",
    "    # Weight ranges from 1.0 (beginning) to 1.5 (end)\n",
    "    avg_position = np.mean(ai_positions)\n",
    "    recency_weight = 1.0 + (avg_position * 0.5)\n",
    "    \n",
    "    return recency_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_enhanced_proximity_terms(text, ai_terms, impact_terms, sentiment_terms, window_size=10):\n",
    "    \"\"\"\n",
    "    Find sentiment terms within specified word window of AI + impact term combinations\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    words = text.lower().split()\n",
    "    proximity_matches = []\n",
    "    \n",
    "    # Find positions of AI and impact terms\n",
    "    ai_positions = []\n",
    "    impact_positions = []\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        for ai_term in ai_terms:\n",
    "            if ai_term.lower() in word:\n",
    "                ai_positions.append(i)\n",
    "        for impact_term in impact_terms:\n",
    "            if impact_term.lower() in word:\n",
    "                impact_positions.append(i)\n",
    "    \n",
    "    # For each AI-impact pair, find sentiment terms within window\n",
    "    for ai_pos in ai_positions:\n",
    "        for impact_pos in impact_positions:\n",
    "            if abs(ai_pos - impact_pos) <= window_size:\n",
    "                # Look for sentiment terms in the window around these positions\n",
    "                window_start = max(0, min(ai_pos, impact_pos) - window_size)\n",
    "                window_end = min(len(words), max(ai_pos, impact_pos) + window_size)\n",
    "                \n",
    "                window_text = ' '.join(words[window_start:window_end])\n",
    "                \n",
    "                # Check for sentiment terms in this window\n",
    "                for term in sentiment_terms:\n",
    "                    if term.lower() in window_text:\n",
    "                        proximity_matches.append({\n",
    "                            'term': term,\n",
    "                            'ai_pos': ai_pos,\n",
    "                            'impact_pos': impact_pos,\n",
    "                            'distance': abs(ai_pos - impact_pos),\n",
    "                            'window_text': window_text\n",
    "                        })\n",
    "    \n",
    "    return proximity_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_sentiment_analysis(text, positive_terms, negative_terms, industry=None):\n",
    "    \"\"\"\n",
    "    Enhanced sentiment analysis with all improvements\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'overall': 0,\n",
    "            'finbert': 0,\n",
    "            'lexicon_normalized': 0,\n",
    "            'proximity_enhanced': 0,\n",
    "            'recency_weighted': 0,\n",
    "            'negation_adjusted': 0\n",
    "        }\n",
    "    \n",
    "    # 1. FinBERT Base Sentiment (replacing TextBlob)\n",
    "    finbert_sentiment = finbert_analyzer.analyze_sentiment(text)\n",
    "    \n",
    "    # 2. Normalized Lexicon Analysis\n",
    "    text_lower = text.lower()\n",
    "    word_count = len(text_lower.split())\n",
    "    \n",
    "    # Calculate raw lexicon scores\n",
    "    positive_score = 0\n",
    "    positive_matches = 0\n",
    "    negative_score = 0\n",
    "    negative_matches = 0\n",
    "    \n",
    "    all_sentiment_terms = list(positive_terms.keys()) + list(negative_terms.keys())\n",
    "    \n",
    "    # Detect negated terms\n",
    "    negated_terms = detect_negations(text, all_sentiment_terms)\n",
    "    \n",
    "    # Score positive terms\n",
    "    for term, value in positive_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            # Apply negation adjustment\n",
    "            if term in negated_terms:\n",
    "                value *= -0.5  # Flip and reduce intensity\n",
    "            positive_matches += count\n",
    "            positive_score += value * count\n",
    "    \n",
    "    # Score negative terms  \n",
    "    for term, value in negative_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            # Apply negation adjustment\n",
    "            if term in negated_terms:\n",
    "                value *= -0.5  # Reduce negative intensity\n",
    "            negative_matches += count\n",
    "            negative_score += value * count\n",
    "    \n",
    "    # 3. Normalize by text length (using log to avoid over-penalizing long texts)\n",
    "    total_matches = positive_matches + negative_matches\n",
    "    if total_matches > 0 and word_count > 0:\n",
    "        normalization_factor = np.log(word_count + 1)  # +1 to avoid log(0)\n",
    "        lexicon_normalized = (positive_score + negative_score) / (total_matches * normalization_factor)\n",
    "    else:\n",
    "        lexicon_normalized = 0\n",
    "    \n",
    "    # 4. Enhanced Proximity Analysis\n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning', 'automation', 'algorithm']\n",
    "    impact_terms = ['job', 'work', 'employee', 'career', 'industry', 'employment', 'worker']\n",
    "    \n",
    "    proximity_matches = find_enhanced_proximity_terms(\n",
    "        text, ai_terms, impact_terms, all_sentiment_terms, window_size=10\n",
    "    )\n",
    "    \n",
    "    proximity_enhanced = 0\n",
    "    if proximity_matches:\n",
    "        proximity_scores = []\n",
    "        for match in proximity_matches:\n",
    "            # Weight by inverse distance (closer = higher weight)\n",
    "            distance_weight = 1.0 / (match['distance'] + 1)\n",
    "            \n",
    "            # Get sentiment of this specific term\n",
    "            term = match['term']\n",
    "            if term in positive_terms:\n",
    "                term_sentiment = positive_terms[term]\n",
    "            elif term in negative_terms:\n",
    "                term_sentiment = negative_terms[term]\n",
    "            else:\n",
    "                term_sentiment = 0\n",
    "            \n",
    "            weighted_sentiment = term_sentiment * distance_weight\n",
    "            proximity_scores.append(weighted_sentiment)\n",
    "        \n",
    "        proximity_enhanced = np.mean(proximity_scores)\n",
    "    \n",
    "    # 5. Recency Weighting\n",
    "    recency_weight = calculate_recency_weight(text, ai_terms)\n",
    "    recency_weighted = lexicon_normalized * recency_weight\n",
    "    \n",
    "    # 6. Negation Adjustment Score\n",
    "    negation_penalty = len(negated_terms) * 0.1  # Small penalty for each negated term\n",
    "    negation_adjusted = lexicon_normalized - negation_penalty\n",
    "    \n",
    "    # 7. New Combined Score Formula\n",
    "    # Weighted combination with more sophisticated weighting\n",
    "    weights = {\n",
    "        'finbert': 0.35,           # Strong weight for BERT\n",
    "        'lexicon_normalized': 0.25, # Normalized lexicon\n",
    "        'proximity_enhanced': 0.25, # Enhanced proximity\n",
    "        'recency_weighted': 0.10,   # Recency boost\n",
    "        'negation_adjusted': 0.05   # Negation handling\n",
    "    }\n",
    "    \n",
    "    overall_sentiment = (\n",
    "        weights['finbert'] * finbert_sentiment +\n",
    "        weights['lexicon_normalized'] * lexicon_normalized +\n",
    "        weights['proximity_enhanced'] * proximity_enhanced +\n",
    "        weights['recency_weighted'] * (recency_weighted - lexicon_normalized) +  # Only the boost\n",
    "        weights['negation_adjusted'] * (negation_adjusted - lexicon_normalized)   # Only the adjustment\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_sentiment,\n",
    "        'finbert': finbert_sentiment,\n",
    "        'lexicon_normalized': lexicon_normalized,\n",
    "        'proximity_enhanced': proximity_enhanced,\n",
    "        'recency_weighted': recency_weighted,\n",
    "        'negation_adjusted': negation_adjusted,\n",
    "        'word_count': word_count,\n",
    "        'proximity_matches_count': len(proximity_matches),\n",
    "        'negated_terms_count': len(negated_terms)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to integrate with your existing pipeline\n",
    "def add_enhanced_features_to_dataset(df, dictionaries):\n",
    "    \"\"\"\n",
    "    Enhanced version of your add_features_to_dataset function\n",
    "    \"\"\"\n",
    "    print(\"Adding enhanced features to dataset...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Dictionaries\n",
    "    sentiment_dict = dictionaries['sentiment']\n",
    "    industry_dict = dictionaries['industry']\n",
    "    job_dict = dictionaries['job']\n",
    "    technology_dict = dictionaries['technology']\n",
    "    \n",
    "    # Existing feature detection (keep your current implementations)\n",
    "    print(\"Detecting industries and jobs...\")\n",
    "    df_enhanced['detected_industries'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_industries(\n",
    "            x, \n",
    "            industry_dict['industry_terms'], \n",
    "            industry_dict['industry_term_weights']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_enhanced['detected_jobs'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_jobs(x, job_dict['job_terms'])\n",
    "    )\n",
    "    \n",
    "    print(\"Identifying AI technologies...\")\n",
    "    df_enhanced['ai_technologies'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: identify_technologies(\n",
    "            x, \n",
    "            technology_dict['technology_terms'],\n",
    "            technology_dict['ai_models']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"Extracting organizations...\")\n",
    "    df_enhanced['top_organizations'] = df_enhanced['cleaned_text'].apply(extract_organizations)\n",
    "    \n",
    "    # ENHANCED SENTIMENT ANALYSIS\n",
    "    print(\"Analyzing sentiment with enhanced model...\")\n",
    "    df_enhanced['enhanced_sentiment_scores'] = df_enhanced.apply(\n",
    "        lambda x: enhanced_sentiment_analysis(\n",
    "            x['cleaned_text'],\n",
    "            sentiment_dict['positive_terms'],\n",
    "            sentiment_dict['negative_terms'],\n",
    "            x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract enhanced sentiment scores\n",
    "    df_enhanced['sentiment_overall_enhanced'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['overall'])\n",
    "    df_enhanced['sentiment_finbert'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['finbert'])\n",
    "    df_enhanced['sentiment_lexicon_normalized'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['lexicon_normalized'])\n",
    "    df_enhanced['sentiment_proximity_enhanced'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['proximity_enhanced'])\n",
    "    df_enhanced['sentiment_recency_weighted'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['recency_weighted'])\n",
    "    df_enhanced['sentiment_negation_adjusted'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['negation_adjusted'])\n",
    "    \n",
    "    # Additional metadata\n",
    "    df_enhanced['article_word_count'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['word_count'])\n",
    "    df_enhanced['proximity_matches_count'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['proximity_matches_count'])\n",
    "    df_enhanced['negated_terms_count'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['negated_terms_count'])\n",
    "    \n",
    "    # Add primary industry and job (keep your existing logic)\n",
    "    df_enhanced['primary_industry'] = df_enhanced['detected_industries'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    df_enhanced['primary_job'] = df_enhanced['detected_jobs'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated pipeline function\n",
    "def run_enhanced_pipeline():\n",
    "    \"\"\"\n",
    "    Enhanced version of your pipeline with improved sentiment analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced pipeline...\")\n",
    "    \n",
    "    # Load the topic data\n",
    "    df = load_from_cache('data_with_topics.pkl')\n",
    "    if df is None:\n",
    "        print(\"ERROR: Could not load data from data_with_topics.pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded data with {len(df)} articles\")\n",
    "    \n",
    "    # Create the dictionaries (using your existing function)\n",
    "    dictionaries = create_dictionaries()\n",
    "    \n",
    "    # Add enhanced features to the dataset\n",
    "    df_enhanced = add_enhanced_features_to_dataset(df, dictionaries)\n",
    "    \n",
    "    # Save enhanced dataset\n",
    "    save_to_cache(df_enhanced, 'enhanced_data_with_finbert_features.pkl')\n",
    "    \n",
    "    print(\"Enhanced pipeline complete!\")\n",
    "    print(\"Enhanced data saved to 'enhanced_data_with_finbert_features.pkl'\")\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache directory: /Users/casey/Documents/GitHub/AI_impact_employment/cache\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cache directory\n",
    "cache_dir = \"cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"Using cache directory: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "def get_cache_path(filename):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def save_to_cache(obj, filename):\n",
    "    \"\"\"Save object to cache\"\"\"\n",
    "    with open(get_cache_path(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved {filename} to cache\")\n",
    "\n",
    "def load_from_cache(filename):\n",
    "    \"\"\"Load object from cache if it exists\"\"\"\n",
    "    cache_path = get_cache_path(filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# Dictionary Functions\n",
    "\n",
    "# I created a set of weighted keywords related to positive and negative sentiment and grouped them by themes.\n",
    "# I will use this to score how positively or negatively AI is being discussed.\n",
    "def create_sentiment_dictionaries():\n",
    "    print(\"Creating sentiment dictionaries...\")\n",
    "    \n",
    "    # Positive terms related to AI in the workplace context\n",
    "    positive_terms = {\n",
    "        # Opportunity and Growth\n",
    "        'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7,\n",
    "        'growth': 0.7, 'advancement': 0.8, 'upskill': 0.9, 'progress': 0.7,\n",
    "        'potential': 0.5, 'revolutionize': 0.8, 'transform': 0.7,\n",
    "\n",
    "        # Productivity and Efficiency\n",
    "        'efficiency': 0.8, 'productivity': 0.8, 'streamline': 0.7,\n",
    "        'optimize': 0.7, 'accelerate': 0.6, 'automate': 0.6,\n",
    "\n",
    "        # Collaboration and Assistance\n",
    "        'assist': 0.6, 'empower': 0.9, 'collaborate': 0.7, 'partnership': 0.6,\n",
    "        'complement': 0.7, 'teamwork': 0.7, 'support': 0.6, 'aid': 0.6,\n",
    "\n",
    "        # Solution and Benefit\n",
    "        'solution': 0.6, 'benefit': 0.8, 'advantage': 0.7, 'value': 0.6,\n",
    "        'solve': 0.7, 'facilitate': 0.6, 'enable': 0.7,\n",
    "\n",
    "        # Innovation and Creation\n",
    "        'innovation': 0.9, 'create': 0.6, 'invent': 0.7, 'develop': 0.6,\n",
    "        'pioneer': 0.8, 'breakthrough': 0.9, 'novel': 0.7\n",
    "    }\n",
    "\n",
    "    # Negative terms related to AI in the workplace context\n",
    "    negative_terms = {\n",
    "        # Job Loss and Replacement\n",
    "        'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'substitute': -0.7,\n",
    "        'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, 'redundant': -0.8,\n",
    "        'downsizing': -0.8, 'obsolete': -0.8, 'outdated': -0.7,\n",
    "\n",
    "        # Risk and Threat\n",
    "        'threaten': -0.7, 'risk': -0.6, 'danger': -0.7, 'concern': -0.5,\n",
    "        'worry': -0.6, 'fear': -0.7, 'threat': -0.8, 'harmful': -0.8,\n",
    "\n",
    "        # Problems and Challenges\n",
    "        'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4, 'difficulty': -0.5,\n",
    "        'obstacle': -0.5, 'hurdle': -0.4, 'barrier': -0.5,\n",
    "\n",
    "        # Social Issues\n",
    "        'inequality': -0.7, 'bias': -0.7, 'discrimination': -0.8, 'unfair': -0.7,\n",
    "        'disparity': -0.7, 'divide': -0.6, 'exclusion': -0.7,\n",
    "\n",
    "        # Control and Privacy\n",
    "        'surveillance': -0.8, 'monitor': -0.6, 'control': -0.6, 'invasion': -0.7,\n",
    "        'privacy': -0.7, 'intrusive': -0.7, 'oversight': -0.5\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'positive_terms': positive_terms,\n",
    "        'negative_terms': negative_terms\n",
    "    }\n",
    "\n",
    "# I created a set of keywords that belong to certain industries like healthcare, finance and manufacturing.\n",
    "def create_industry_dictionaries():\n",
    "    \"\"\"Create comprehensive industry dictionaries using domain knowledge\"\"\"\n",
    "    print(\"Creating industry dictionaries...\")\n",
    "    \n",
    "    industry_terms = {\n",
    "        'healthcare': [\n",
    "            'doctor', 'physician', 'nurse', 'hospital', 'clinic', 'patient', 'care',\n",
    "            'medical', 'healthcare', 'health care', 'medicine', 'pharma', 'clinical'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'bank', 'banking', 'investment', 'investor', 'loan', 'credit', \n",
    "            'financial', 'finance', 'trading', 'insurance', 'fintech'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory', 'manufacturing', 'production', 'assembly', 'supply chain',\n",
    "            'industrial', 'automotive', 'machinery', 'robotics', 'automation'\n",
    "        ],\n",
    "\n",
    "        'retail': [\n",
    "            'store', 'shop', 'retail', 'e-commerce', 'customer', 'consumer',\n",
    "            'inventory', 'merchandising', 'commerce', 'shopping'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'school', 'university', 'college', 'student', 'teacher', 'professor',\n",
    "            'education', 'learning', 'teaching', 'training', 'academic'\n",
    "        ],\n",
    "\n",
    "        'technology': [\n",
    "            'software', 'hardware', 'tech', 'technology', 'computer', 'digital',\n",
    "            'it', 'internet', 'web', 'app', 'computing', 'cloud'\n",
    "        ],\n",
    "\n",
    "        'media': [\n",
    "            'media', 'news', 'entertainment', 'publishing', 'content', \n",
    "            'social media', 'journalist', 'writing', 'advertising'\n",
    "        ],\n",
    "\n",
    "        'legal': [\n",
    "            'legal', 'lawyer', 'attorney', 'law firm', 'regulatory', 'compliance',\n",
    "            'court', 'litigation', 'judge', 'justice', 'contract'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # I also gave each term a weight, so more specific terms like hospital or e-commerce have a higher weight.\n",
    "    # This help improve precision when detecting what industry the article talks about.\n",
    "    industry_term_weights = {\n",
    "        'healthcare': {'hospital': 5, 'doctor': 4, 'patient': 3, 'medical': 2, 'healthcare': 5},\n",
    "        'finance': {'bank': 5, 'investment': 4, 'financial': 3, 'loan': 2, 'finance': 5},\n",
    "        'manufacturing': {'factory': 5, 'manufacturing': 5, 'production': 4, 'assembly': 3},\n",
    "        'retail': {'store': 4, 'retail': 5, 'e-commerce': 5, 'consumer': 3},\n",
    "        'education': {'school': 5, 'university': 5, 'student': 4, 'education': 5},\n",
    "        'technology': {'software': 4, 'tech': 5, 'technology': 5, 'digital': 3},\n",
    "        'media': {'media': 5, 'news': 4, 'content': 3, 'publishing': 4},\n",
    "        'legal': {'lawyer': 5, 'legal': 5, 'law': 4, 'attorney': 5}\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'industry_terms': industry_terms,\n",
    "        'industry_term_weights': industry_term_weights\n",
    "    }\n",
    "\n",
    "# Same idea as industries but for job categories like engineering, creative and healthcare.\n",
    "# Links AI mentions to who might be affected like teacher or developer.\n",
    "def create_job_dictionaries():\n",
    "    print(\"Creating job dictionaries...\")\n",
    "    \n",
    "    job_terms = {\n",
    "        'management': [\n",
    "            'ceo', 'chief executive', 'cfo', 'cio', 'cto', 'coo', 'executive',\n",
    "            'manager', 'supervisor', 'director', 'leadership', 'administration'\n",
    "        ],\n",
    "\n",
    "        'engineering': [\n",
    "            'engineer', 'developer', 'programmer', 'coder', 'data scientist',\n",
    "            'machine learning engineer', 'ai engineer', 'software engineer',\n",
    "            'technical', 'architect', 'DevOps'\n",
    "        ],\n",
    "\n",
    "        'creative': [\n",
    "            'designer', 'writer', 'artist', 'content creator', 'creative',\n",
    "            'marketer', 'marketing', 'advertiser', 'author', 'editor'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'teacher', 'professor', 'instructor', 'educator', 'faculty',\n",
    "            'academic', 'trainer', 'teaching', 'tutor', 'lecturer'\n",
    "        ],\n",
    "\n",
    "        'healthcare': [\n",
    "            'doctor', 'nurse', 'physician', 'surgeon', 'medical professional',\n",
    "            'pharmacist', 'therapist', 'healthcare worker', 'clinician'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'banker', 'accountant', 'financial analyst', 'trader', 'investor',\n",
    "            'broker', 'financial advisor', 'auditor', 'actuary'\n",
    "        ],\n",
    "\n",
    "        'service': [\n",
    "            'customer service', 'retail worker', 'sales associate', 'cashier',\n",
    "            'receptionist', 'assistant', 'representative', 'clerk'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory worker', 'machine operator', 'assembler', 'production worker',\n",
    "            'technician', 'mechanic', 'quality control', 'maintenance'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'job_terms': job_terms\n",
    "    }\n",
    "\n",
    "# To cover different AI domains like NLP, computer vision and robotics. I also included real-world AI model names like GPT for mention of tools.\n",
    "def create_technology_dictionaries():\n",
    "    print(\"Creating technology dictionaries...\")\n",
    "    \n",
    "    technology_terms = {\n",
    "        'machine_learning': [\n",
    "            'machine learning', 'ml', 'artificial intelligence', 'ai', 'algorithm',\n",
    "            'deep learning', 'neural network', 'data science'\n",
    "        ],\n",
    "\n",
    "        'nlp': [\n",
    "            'natural language processing', 'nlp', 'language model', 'llm',\n",
    "            'large language model', 'chatbot', 'gpt', 'bert'\n",
    "        ],\n",
    "\n",
    "        'computer_vision': [\n",
    "            'computer vision', 'image recognition', 'object detection',\n",
    "            'facial recognition', 'image processing'\n",
    "        ],\n",
    "\n",
    "        'robotics': [\n",
    "            'robot', 'robotics', 'automation', 'autonomous', 'self-driving',\n",
    "            'robotic process automation', 'rpa'\n",
    "        ],\n",
    "\n",
    "        'ai_infrastructure': [\n",
    "            'gpu', 'cloud computing', 'edge computing', 'federated learning',\n",
    "            'ai chip', 'compute', 'transformer'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # AI product models\n",
    "    ai_models = [\n",
    "        'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'dall-e', 'bard', 'palm',\n",
    "        'llama', 'claude', 'stable diffusion', 'midjourney', 'gemini'\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'technology_terms': technology_terms,\n",
    "        'ai_models': ai_models\n",
    "    }\n",
    "\n",
    "# Feature Extraction Functions\n",
    "\n",
    "# I created a function to detect industries mentioned in the article text. It scans the articles for industry keywords and scores each category.\n",
    "# I multiply frequency * weight * term length weight, this so longer and more precise terms count more.\n",
    "# The it will return the most likely industries mentioned in the article.\n",
    "def detect_industries(text, industry_terms, industry_term_weights=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Counting the occurrences of each category's keywords with weights.\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in industry_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Term-specific weight.\n",
    "                weight = 1.0\n",
    "                if industry_term_weights and category in industry_term_weights and term in industry_term_weights[category]:\n",
    "                    weight = industry_term_weights[category][term]\n",
    "                \n",
    "                # Applying the additional weight for longer and more specific terms.\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Final score\n",
    "                score = count * weight * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # By descending.\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Gives the categories.\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "# I created a function to detect job categories mentioned in the article text. This is for identifying who the article is referring to engineers, nurses, etc.\n",
    "def detect_jobs(text, job_terms):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Counting occurrences of each category keywords\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in job_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Additional weight for longer and more specific terms.\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Final score\n",
    "                score = count * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # Descending order\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Returns the categories\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "# This finds the mentions of AI technologies in the text.mI wanted this to track whether\n",
    "# the article is talking about general tech like machine learning or specific tools like ChatGPT.\n",
    "def identify_technologies(text, technology_terms, ai_models):\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    found_techs = {}\n",
    "    \n",
    "    # Technology categories\n",
    "    for tech_category, keywords in technology_terms.items():\n",
    "        matched_keywords = [k for k in keywords if k in text_lower]\n",
    "        if matched_keywords:\n",
    "            # Sort by length, since longer terms are typically more specific\n",
    "            matched_keywords.sort(key=len, reverse=True)\n",
    "            found_techs[tech_category] = matched_keywords\n",
    "    \n",
    "    # AI models\n",
    "    found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "    if found_models:\n",
    "        found_techs['specific_models'] = found_models\n",
    "    \n",
    "    return found_techs\n",
    "\n",
    "# This extracts organization names from the text. I used a simple heuristic to find known AI companies.\n",
    "# If the article mentions them, I extract them and it helps link the content to real world actors.\n",
    "def extract_organizations(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Major AI companies and organizations\n",
    "    known_orgs = [\n",
    "        'OpenAI', 'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Facebook',\n",
    "        'IBM', 'Anthropic', 'NVIDIA', 'Intel', 'AMD', 'Tesla', 'DeepMind'\n",
    "    ]\n",
    "    \n",
    "    # Find the known organizations in the text\n",
    "    found_orgs = []\n",
    "    for org in known_orgs:\n",
    "        if org.lower() in text.lower():\n",
    "            found_orgs.append(org)\n",
    "    \n",
    "    # Top 5\n",
    "    return found_orgs[:5]\n",
    "\n",
    "# This is a custom sentiment function that combines:\n",
    "# TextBlob sentiment analysis \n",
    "# My own weighted keyword scores which make it domain-specific\n",
    "# Sentence level sentiment where AI and impact keywords are close together\n",
    "# I combine all of these into a weighted score\n",
    "def analyze_sentiment(text, positive_terms, negative_terms, industry=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'overall': 0,\n",
    "            'base': 0,\n",
    "            'lexicon': 0,\n",
    "            'proximity': 0,\n",
    "            'industry': 0\n",
    "        }\n",
    "    \n",
    "    # Base sentiment from TextBlob\n",
    "    base_sentiment = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    # Domain specific approach\n",
    "    text_lower = text.lower()\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # 1. Calculate overall sentiment using domain specific lexicon\n",
    "    positive_matches = 0\n",
    "    positive_score = 0\n",
    "    negative_matches = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    # Counting and scoring the positive terms\n",
    "    for term, value in positive_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            positive_matches += count\n",
    "            positive_score += value * count\n",
    "    \n",
    "    # Counting and scoring the negative terms\n",
    "    for term, value in negative_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            negative_matches += count\n",
    "            negative_score += value * count\n",
    "    \n",
    "    # 2. Calculating the proximity between AI and impact terms\n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning']\n",
    "    impact_terms = ['job', 'work', 'employee', 'career', 'industry']\n",
    "    \n",
    "    proximity_score = 0\n",
    "    proximity_count = 0\n",
    "    \n",
    "    # Checking sentences containing both AI and impact terms\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        has_ai = any(term in sentence for term in ai_terms)\n",
    "        has_impact = any(term in sentence for term in impact_terms)\n",
    "        \n",
    "        if has_ai and has_impact:\n",
    "            # Sentiment for this sentence\n",
    "            sent_sentiment = TextBlob(sentence).sentiment.polarity\n",
    "            proximity_score += sent_sentiment\n",
    "            proximity_count += 1\n",
    "    \n",
    "    # 3. Industry specific sentiment\n",
    "    industry_sentiment = 0\n",
    "    \n",
    "    # 4. Final weighted sentiment scores\n",
    "    lexicon_sentiment = 0\n",
    "    if (positive_matches + negative_matches) > 0:\n",
    "        lexicon_sentiment = (positive_score + negative_score) / (positive_matches + negative_matches)\n",
    "    \n",
    "    proximity_sentiment = 0\n",
    "    if proximity_count > 0:\n",
    "        proximity_sentiment = proximity_score / proximity_count\n",
    "    \n",
    "    # Final weighted score\n",
    "    weights = {\n",
    "        'base': 0.2,\n",
    "        'lexicon': 0.4,\n",
    "        'proximity': 0.3,\n",
    "        'industry': 0.1\n",
    "    }\n",
    "    \n",
    "    final_sentiment = (\n",
    "        weights['base'] * base_sentiment +\n",
    "        weights['lexicon'] * lexicon_sentiment +\n",
    "        weights['proximity'] * proximity_sentiment +\n",
    "        weights['industry'] * industry_sentiment\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': final_sentiment,\n",
    "        'base': base_sentiment,\n",
    "        'lexicon': lexicon_sentiment,\n",
    "        'proximity': proximity_sentiment,\n",
    "        'industry': industry_sentiment\n",
    "    }\n",
    "\n",
    "# Main Functions\n",
    "\n",
    "# Ir runs all the setup functions above and returns a dictionary of all terms and weights.\n",
    "def create_dictionaries():\n",
    "    dictionaries = {}\n",
    "    \n",
    "    # 1. Sentiment Dictionaries\n",
    "    dictionaries['sentiment'] = create_sentiment_dictionaries()\n",
    "    \n",
    "    # 2. Industry Dictionaries\n",
    "    dictionaries['industry'] = create_industry_dictionaries()\n",
    "    \n",
    "    # 3. Job Dictionaries\n",
    "    dictionaries['job'] = create_job_dictionaries()\n",
    "    \n",
    "    # 4. Technology Dictionaries\n",
    "    dictionaries['technology'] = create_technology_dictionaries()\n",
    "    \n",
    "    return dictionaries\n",
    "\n",
    "\n",
    "# The function takes the dataset and the dictionaries and adds new features to the dataset.\n",
    "# It detects industries, jobs, technologies, organizations and sentiment scores and returns the enhanced dataset with all the new features.\n",
    "# I added a few more features like primary industry and job.\n",
    "def add_features_to_dataset(df, dictionaries):\n",
    "    print(\"Adding features to dataset...\")\n",
    "    \n",
    "    # Making a copy to avoid modifying the original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Dictionaries\n",
    "    sentiment_dict = dictionaries['sentiment']\n",
    "    industry_dict = dictionaries['industry']\n",
    "    job_dict = dictionaries['job']\n",
    "    technology_dict = dictionaries['technology']\n",
    "    \n",
    "    # Detecting industries\n",
    "    print(\"Detecting industries and jobs...\")\n",
    "    df_enhanced['detected_industries'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_industries(\n",
    "            x, \n",
    "            industry_dict['industry_terms'], \n",
    "            industry_dict['industry_term_weights']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Detecting jobs\n",
    "    df_enhanced['detected_jobs'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_jobs(x, job_dict['job_terms'])\n",
    "    )\n",
    "    \n",
    "    # Identifying technologies\n",
    "    print(\"Identifying AI technologies...\")\n",
    "    df_enhanced['ai_technologies'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: identify_technologies(\n",
    "            x, \n",
    "            technology_dict['technology_terms'],\n",
    "            technology_dict['ai_models']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extracting the organizations\n",
    "    print(\"Extracting organizations...\")\n",
    "    df_enhanced['top_organizations'] = df_enhanced['cleaned_text'].apply(extract_organizations)\n",
    "    \n",
    "    # Analyzing the sentiment\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    df_enhanced['sentiment_scores'] = df_enhanced.apply(\n",
    "        lambda x: analyze_sentiment(\n",
    "            x['cleaned_text'],\n",
    "            sentiment_dict['positive_terms'],\n",
    "            sentiment_dict['negative_terms'],\n",
    "            x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extracting sentiment scores\n",
    "    df_enhanced['sentiment_overall'] = df_enhanced['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "    df_enhanced['sentiment_base'] = df_enhanced['sentiment_scores'].apply(lambda x: x['base'])\n",
    "    df_enhanced['sentiment_lexicon'] = df_enhanced['sentiment_scores'].apply(lambda x: x['lexicon'])\n",
    "    df_enhanced['sentiment_proximity'] = df_enhanced['sentiment_scores'].apply(lambda x: x['proximity'])\n",
    "    df_enhanced['sentiment_industry'] = df_enhanced['sentiment_scores'].apply(lambda x: x['industry'])\n",
    "    \n",
    "    # Adding primary industry and job\n",
    "    df_enhanced['primary_industry'] = df_enhanced['detected_industries'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    df_enhanced['primary_job'] = df_enhanced['detected_jobs'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Runs the pipeline\n",
    "def run_enhancement_pipeline():\n",
    "    print(\"Starting enhancement pipeline...\")\n",
    "    \n",
    "    # Loading the topic data\n",
    "    df = load_from_cache('data_with_topics.pkl')\n",
    "    if df is None:\n",
    "        print(\"ERROR: Could not load data from data_with_topics.pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded data with {len(df)} articles\")\n",
    "    \n",
    "    # Creating the dictionaries\n",
    "    dictionaries = create_dictionaries()\n",
    "    \n",
    "    # Adding the features to the dataset\n",
    "    df_enhanced = add_features_to_dataset(df, dictionaries)\n",
    "    \n",
    "    # Save dataset\n",
    "    save_to_cache(df_enhanced, 'enhanced_data_with_features.pkl')\n",
    "    \n",
    "    print(\"Enhancement pipeline complete!\")\n",
    "    print(\"Enhanced data saved to 'enhanced_data_with_features.pkl'\")\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhancement pipeline...\n",
      "Loaded data with 184391 articles\n",
      "Creating sentiment dictionaries...\n",
      "Creating industry dictionaries...\n",
      "Creating job dictionaries...\n",
      "Creating technology dictionaries...\n",
      "Adding features to dataset...\n",
      "Detecting industries and jobs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     run_enhancement_pipeline()\n",
      "Cell \u001b[0;32mIn[11], line 562\u001b[0m, in \u001b[0;36mrun_enhancement_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    559\u001b[0m dictionaries \u001b[38;5;241m=\u001b[39m create_dictionaries()\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# Adding the features to the dataset\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m df_enhanced \u001b[38;5;241m=\u001b[39m add_features_to_dataset(df, dictionaries)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Save dataset\u001b[39;00m\n\u001b[1;32m    565\u001b[0m save_to_cache(df_enhanced, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_data_with_features.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 498\u001b[0m, in \u001b[0;36madd_features_to_dataset\u001b[0;34m(df, dictionaries)\u001b[0m\n\u001b[1;32m    489\u001b[0m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetected_industries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: detect_industries(\n\u001b[1;32m    491\u001b[0m         x, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m )\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Detecting jobs\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetected_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: detect_jobs(x, job_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_terms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    500\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Identifying technologies\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentifying AI technologies...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 499\u001b[0m, in \u001b[0;36madd_features_to_dataset.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    489\u001b[0m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetected_industries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: detect_industries(\n\u001b[1;32m    491\u001b[0m         x, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m )\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Detecting jobs\u001b[39;00m\n\u001b[1;32m    498\u001b[0m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetected_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: detect_jobs(x, job_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_terms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    500\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Identifying technologies\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentifying AI technologies...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 297\u001b[0m, in \u001b[0;36mdetect_jobs\u001b[0;34m(text, job_terms)\u001b[0m\n\u001b[1;32m    294\u001b[0m count \u001b[38;5;241m=\u001b[39m text_lower\u001b[38;5;241m.\u001b[39mcount(term)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Additional weight for longer and more specific terms.\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     length_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(term) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20.0\u001b[39m)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# Final score\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     score \u001b[38;5;241m=\u001b[39m count \u001b[38;5;241m*\u001b[39m length_weight\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_enhancement_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
