{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to improve each articles analysis with more features that go beyond text. I created dictionaries to detect industries, job roles, technologies, organizations and to perform my custom sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache directory: /Users/casey/Documents/GitHub/AI_impact_employment/cache\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cache directory\n",
    "cache_dir = \"cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"Using cache directory: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "def get_cache_path(filename):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def save_to_cache(obj, filename):\n",
    "    \"\"\"Save object to cache\"\"\"\n",
    "    with open(get_cache_path(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved {filename} to cache\")\n",
    "\n",
    "def load_from_cache(filename):\n",
    "    \"\"\"Load object from cache if it exists\"\"\"\n",
    "    cache_path = get_cache_path(filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# Dictionary Functions\n",
    "\n",
    "# I created a set of weighted keywords related to positive and negative sentiment and grouped them by themes.\n",
    "# I will use this to score how positively or negatively AI is being discussed.\n",
    "def create_sentiment_dictionaries():\n",
    "    print(\"Creating sentiment dictionaries...\")\n",
    "    \n",
    "    # Positive terms related to AI in the workplace context\n",
    "    positive_terms = {\n",
    "        # Opportunity and Growth\n",
    "        'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7,\n",
    "        'growth': 0.7, 'advancement': 0.8, 'upskill': 0.9, 'progress': 0.7,\n",
    "        'potential': 0.5, 'revolutionize': 0.8, 'transform': 0.7,\n",
    "\n",
    "        # Productivity and Efficiency\n",
    "        'efficiency': 0.8, 'productivity': 0.8, 'streamline': 0.7,\n",
    "        'optimize': 0.7, 'accelerate': 0.6, 'automate': 0.6,\n",
    "\n",
    "        # Collaboration and Assistance\n",
    "        'assist': 0.6, 'empower': 0.9, 'collaborate': 0.7, 'partnership': 0.6,\n",
    "        'complement': 0.7, 'teamwork': 0.7, 'support': 0.6, 'aid': 0.6,\n",
    "\n",
    "        # Solution and Benefit\n",
    "        'solution': 0.6, 'benefit': 0.8, 'advantage': 0.7, 'value': 0.6,\n",
    "        'solve': 0.7, 'facilitate': 0.6, 'enable': 0.7,\n",
    "\n",
    "        # Innovation and Creation\n",
    "        'innovation': 0.9, 'create': 0.6, 'invent': 0.7, 'develop': 0.6,\n",
    "        'pioneer': 0.8, 'breakthrough': 0.9, 'novel': 0.7\n",
    "    }\n",
    "\n",
    "    # Negative terms related to AI in the workplace context\n",
    "    negative_terms = {\n",
    "        # Job Loss and Replacement\n",
    "        'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'substitute': -0.7,\n",
    "        'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, 'redundant': -0.8,\n",
    "        'downsizing': -0.8, 'obsolete': -0.8, 'outdated': -0.7,\n",
    "\n",
    "        # Risk and Threat\n",
    "        'threaten': -0.7, 'risk': -0.6, 'danger': -0.7, 'concern': -0.5,\n",
    "        'worry': -0.6, 'fear': -0.7, 'threat': -0.8, 'harmful': -0.8,\n",
    "\n",
    "        # Problems and Challenges\n",
    "        'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4, 'difficulty': -0.5,\n",
    "        'obstacle': -0.5, 'hurdle': -0.4, 'barrier': -0.5,\n",
    "\n",
    "        # Social Issues\n",
    "        'inequality': -0.7, 'bias': -0.7, 'discrimination': -0.8, 'unfair': -0.7,\n",
    "        'disparity': -0.7, 'divide': -0.6, 'exclusion': -0.7,\n",
    "\n",
    "        # Control and Privacy\n",
    "        'surveillance': -0.8, 'monitor': -0.6, 'control': -0.6, 'invasion': -0.7,\n",
    "        'privacy': -0.7, 'intrusive': -0.7, 'oversight': -0.5\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'positive_terms': positive_terms,\n",
    "        'negative_terms': negative_terms\n",
    "    }\n",
    "\n",
    "# I created a set of keywords that belong to certain industries like healthcare, finance and manufacturing.\n",
    "def create_industry_dictionaries():\n",
    "    \"\"\"Create comprehensive industry dictionaries using domain knowledge\"\"\"\n",
    "    print(\"Creating industry dictionaries...\")\n",
    "    \n",
    "    industry_terms = {\n",
    "        'healthcare': [\n",
    "            'doctor', 'physician', 'nurse', 'hospital', 'clinic', 'patient', 'care',\n",
    "            'medical', 'healthcare', 'health care', 'medicine', 'pharma', 'clinical'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'bank', 'banking', 'investment', 'investor', 'loan', 'credit', \n",
    "            'financial', 'finance', 'trading', 'insurance', 'fintech'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory', 'manufacturing', 'production', 'assembly', 'supply chain',\n",
    "            'industrial', 'automotive', 'machinery', 'robotics', 'automation'\n",
    "        ],\n",
    "\n",
    "        'retail': [\n",
    "            'store', 'shop', 'retail', 'e-commerce', 'customer', 'consumer',\n",
    "            'inventory', 'merchandising', 'commerce', 'shopping'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'school', 'university', 'college', 'student', 'teacher', 'professor',\n",
    "            'education', 'learning', 'teaching', 'training', 'academic'\n",
    "        ],\n",
    "\n",
    "        'technology': [\n",
    "            'software', 'hardware', 'tech', 'technology', 'computer', 'digital',\n",
    "            'it', 'internet', 'web', 'app', 'computing', 'cloud'\n",
    "        ],\n",
    "\n",
    "        'media': [\n",
    "            'media', 'news', 'entertainment', 'publishing', 'content', \n",
    "            'social media', 'journalist', 'writing', 'advertising'\n",
    "        ],\n",
    "\n",
    "        'legal': [\n",
    "            'legal', 'lawyer', 'attorney', 'law firm', 'regulatory', 'compliance',\n",
    "            'court', 'litigation', 'judge', 'justice', 'contract'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # I also gave each term a weight, so more specific terms like hospital or e-commerce have a higher weight.\n",
    "    # This help improve precision when detecting what industry the article talks about.\n",
    "    industry_term_weights = {\n",
    "        'healthcare': {'hospital': 5, 'doctor': 4, 'patient': 3, 'medical': 2, 'healthcare': 5},\n",
    "        'finance': {'bank': 5, 'investment': 4, 'financial': 3, 'loan': 2, 'finance': 5},\n",
    "        'manufacturing': {'factory': 5, 'manufacturing': 5, 'production': 4, 'assembly': 3},\n",
    "        'retail': {'store': 4, 'retail': 5, 'e-commerce': 5, 'consumer': 3},\n",
    "        'education': {'school': 5, 'university': 5, 'student': 4, 'education': 5},\n",
    "        'technology': {'software': 4, 'tech': 5, 'technology': 5, 'digital': 3},\n",
    "        'media': {'media': 5, 'news': 4, 'content': 3, 'publishing': 4},\n",
    "        'legal': {'lawyer': 5, 'legal': 5, 'law': 4, 'attorney': 5}\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'industry_terms': industry_terms,\n",
    "        'industry_term_weights': industry_term_weights\n",
    "    }\n",
    "\n",
    "# Same idea as industries but for job categories like engineering, creative and healthcare.\n",
    "# Links AI mentions to who might be affected like teacher or developer.\n",
    "def create_job_dictionaries():\n",
    "    print(\"Creating job dictionaries...\")\n",
    "    \n",
    "    job_terms = {\n",
    "        'management': [\n",
    "            'ceo', 'chief executive', 'cfo', 'cio', 'cto', 'coo', 'executive',\n",
    "            'manager', 'supervisor', 'director', 'leadership', 'administration'\n",
    "        ],\n",
    "\n",
    "        'engineering': [\n",
    "            'engineer', 'developer', 'programmer', 'coder', 'data scientist',\n",
    "            'machine learning engineer', 'ai engineer', 'software engineer',\n",
    "            'technical', 'architect', 'DevOps'\n",
    "        ],\n",
    "\n",
    "        'creative': [\n",
    "            'designer', 'writer', 'artist', 'content creator', 'creative',\n",
    "            'marketer', 'marketing', 'advertiser', 'author', 'editor'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'teacher', 'professor', 'instructor', 'educator', 'faculty',\n",
    "            'academic', 'trainer', 'teaching', 'tutor', 'lecturer'\n",
    "        ],\n",
    "\n",
    "        'healthcare': [\n",
    "            'doctor', 'nurse', 'physician', 'surgeon', 'medical professional',\n",
    "            'pharmacist', 'therapist', 'healthcare worker', 'clinician'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'banker', 'accountant', 'financial analyst', 'trader', 'investor',\n",
    "            'broker', 'financial advisor', 'auditor', 'actuary'\n",
    "        ],\n",
    "\n",
    "        'service': [\n",
    "            'customer service', 'retail worker', 'sales associate', 'cashier',\n",
    "            'receptionist', 'assistant', 'representative', 'clerk'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory worker', 'machine operator', 'assembler', 'production worker',\n",
    "            'technician', 'mechanic', 'quality control', 'maintenance'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'job_terms': job_terms\n",
    "    }\n",
    "\n",
    "# To cover different AI domains like NLP, computer vision and robotics. I also included real-world AI model names like GPT for mention of tools.\n",
    "def create_technology_dictionaries():\n",
    "    print(\"Creating technology dictionaries...\")\n",
    "    \n",
    "    technology_terms = {\n",
    "        'machine_learning': [\n",
    "            'machine learning', 'ml', 'artificial intelligence', 'ai', 'algorithm',\n",
    "            'deep learning', 'neural network', 'data science'\n",
    "        ],\n",
    "\n",
    "        'nlp': [\n",
    "            'natural language processing', 'nlp', 'language model', 'llm',\n",
    "            'large language model', 'chatbot', 'gpt', 'bert'\n",
    "        ],\n",
    "\n",
    "        'computer_vision': [\n",
    "            'computer vision', 'image recognition', 'object detection',\n",
    "            'facial recognition', 'image processing'\n",
    "        ],\n",
    "\n",
    "        'robotics': [\n",
    "            'robot', 'robotics', 'automation', 'autonomous', 'self-driving',\n",
    "            'robotic process automation', 'rpa'\n",
    "        ],\n",
    "\n",
    "        'ai_infrastructure': [\n",
    "            'gpu', 'cloud computing', 'edge computing', 'federated learning',\n",
    "            'ai chip', 'compute', 'transformer'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # AI product models\n",
    "    ai_models = [\n",
    "        'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'dall-e', 'bard', 'palm',\n",
    "        'llama', 'claude', 'stable diffusion', 'midjourney', 'gemini'\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'technology_terms': technology_terms,\n",
    "        'ai_models': ai_models\n",
    "    }\n",
    "\n",
    "# Feature Extraction Functions\n",
    "\n",
    "# I created a function to detect industries mentioned in the article text. It scans the articles for industry keywords and scores each category.\n",
    "# I multiply frequency * weight * term length weight, this so longer and more precise terms count more.\n",
    "# The it will return the most likely industries mentioned in the article.\n",
    "def detect_industries(text, industry_terms, industry_term_weights=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Counting the occurrences of each category's keywords with weights.\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in industry_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Term-specific weight.\n",
    "                weight = 1.0\n",
    "                if industry_term_weights and category in industry_term_weights and term in industry_term_weights[category]:\n",
    "                    weight = industry_term_weights[category][term]\n",
    "                \n",
    "                # Applying the additional weight for longer and more specific terms.\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Final score\n",
    "                score = count * weight * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # By descending.\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Gives the categories.\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "# I created a function to detect job categories mentioned in the article text. This is for identifying who the article is referring to engineers, nurses, etc.\n",
    "def detect_jobs(text, job_terms):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Counting occurrences of each category keywords\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in job_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Additional weight for longer and more specific terms.\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Final score\n",
    "                score = count * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # Descending order\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Returns the categories\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "# This finds the mentions of AI technologies in the text.mI wanted this to track whether\n",
    "# the article is talking about general tech like machine learning or specific tools like ChatGPT.\n",
    "def identify_technologies(text, technology_terms, ai_models):\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    found_techs = {}\n",
    "    \n",
    "    # Technology categories\n",
    "    for tech_category, keywords in technology_terms.items():\n",
    "        matched_keywords = [k for k in keywords if k in text_lower]\n",
    "        if matched_keywords:\n",
    "            # Sort by length, since longer terms are typically more specific\n",
    "            matched_keywords.sort(key=len, reverse=True)\n",
    "            found_techs[tech_category] = matched_keywords\n",
    "    \n",
    "    # AI models\n",
    "    found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "    if found_models:\n",
    "        found_techs['specific_models'] = found_models\n",
    "    \n",
    "    return found_techs\n",
    "\n",
    "# This extracts organization names from the text. I used a simple heuristic to find known AI companies.\n",
    "# If the article mentions them, I extract them and it helps link the content to real world actors.\n",
    "def extract_organizations(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Major AI companies and organizations\n",
    "    known_orgs = [\n",
    "        'OpenAI', 'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Facebook',\n",
    "        'IBM', 'Anthropic', 'NVIDIA', 'Intel', 'AMD', 'Tesla', 'DeepMind'\n",
    "    ]\n",
    "    \n",
    "    # Find the known organizations in the text\n",
    "    found_orgs = []\n",
    "    for org in known_orgs:\n",
    "        if org.lower() in text.lower():\n",
    "            found_orgs.append(org)\n",
    "    \n",
    "    # Top 5\n",
    "    return found_orgs[:5]\n",
    "\n",
    "# This is a custom sentiment function that combines:\n",
    "# TextBlob sentiment analysis \n",
    "# My own weighted keyword scores which make it domain-specific\n",
    "# Sentence level sentiment where AI and impact keywords are close together\n",
    "# I combine all of these into a weighted score\n",
    "def analyze_sentiment(text, positive_terms, negative_terms, industry=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'overall': 0,\n",
    "            'base': 0,\n",
    "            'lexicon': 0,\n",
    "            'proximity': 0,\n",
    "            'industry': 0\n",
    "        }\n",
    "    \n",
    "    # Base sentiment from TextBlob\n",
    "    base_sentiment = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    # Domain specific approach\n",
    "    text_lower = text.lower()\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # 1. Calculate overall sentiment using domain specific lexicon\n",
    "    positive_matches = 0\n",
    "    positive_score = 0\n",
    "    negative_matches = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    # Counting and scoring the positive terms\n",
    "    for term, value in positive_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            positive_matches += count\n",
    "            positive_score += value * count\n",
    "    \n",
    "    # Counting and scoring the negative terms\n",
    "    for term, value in negative_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            negative_matches += count\n",
    "            negative_score += value * count\n",
    "    \n",
    "    # 2. Calculating the proximity between AI and impact terms\n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning']\n",
    "    impact_terms = ['job', 'work', 'employee', 'career', 'industry']\n",
    "    \n",
    "    proximity_score = 0\n",
    "    proximity_count = 0\n",
    "    \n",
    "    # Checking sentences containing both AI and impact terms\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        has_ai = any(term in sentence for term in ai_terms)\n",
    "        has_impact = any(term in sentence for term in impact_terms)\n",
    "        \n",
    "        if has_ai and has_impact:\n",
    "            # Sentiment for this sentence\n",
    "            sent_sentiment = TextBlob(sentence).sentiment.polarity\n",
    "            proximity_score += sent_sentiment\n",
    "            proximity_count += 1\n",
    "    \n",
    "    # 3. Industry specific sentiment\n",
    "    industry_sentiment = 0\n",
    "    \n",
    "    # 4. Final weighted sentiment scores\n",
    "    lexicon_sentiment = 0\n",
    "    if (positive_matches + negative_matches) > 0:\n",
    "        lexicon_sentiment = (positive_score + negative_score) / (positive_matches + negative_matches)\n",
    "    \n",
    "    proximity_sentiment = 0\n",
    "    if proximity_count > 0:\n",
    "        proximity_sentiment = proximity_score / proximity_count\n",
    "    \n",
    "    # Final weighted score\n",
    "    weights = {\n",
    "        'base': 0.2,\n",
    "        'lexicon': 0.4,\n",
    "        'proximity': 0.3,\n",
    "        'industry': 0.1\n",
    "    }\n",
    "    \n",
    "    final_sentiment = (\n",
    "        weights['base'] * base_sentiment +\n",
    "        weights['lexicon'] * lexicon_sentiment +\n",
    "        weights['proximity'] * proximity_sentiment +\n",
    "        weights['industry'] * industry_sentiment\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': final_sentiment,\n",
    "        'base': base_sentiment,\n",
    "        'lexicon': lexicon_sentiment,\n",
    "        'proximity': proximity_sentiment,\n",
    "        'industry': industry_sentiment\n",
    "    }\n",
    "\n",
    "# Main Functions\n",
    "\n",
    "# Ir runs all the setup functions above and returns a dictionary of all terms and weights.\n",
    "def create_dictionaries():\n",
    "    dictionaries = {}\n",
    "    \n",
    "    # 1. Sentiment Dictionaries\n",
    "    dictionaries['sentiment'] = create_sentiment_dictionaries()\n",
    "    \n",
    "    # 2. Industry Dictionaries\n",
    "    dictionaries['industry'] = create_industry_dictionaries()\n",
    "    \n",
    "    # 3. Job Dictionaries\n",
    "    dictionaries['job'] = create_job_dictionaries()\n",
    "    \n",
    "    # 4. Technology Dictionaries\n",
    "    dictionaries['technology'] = create_technology_dictionaries()\n",
    "    \n",
    "    return dictionaries\n",
    "\n",
    "\n",
    "# The function takes the dataset and the dictionaries and adds new features to the dataset.\n",
    "# It detects industries, jobs, technologies, organizations and sentiment scores and returns the enhanced dataset with all the new features.\n",
    "# I added a few more features like primary industry and job.\n",
    "def add_features_to_dataset(df, dictionaries):\n",
    "    print(\"Adding features to dataset...\")\n",
    "    \n",
    "    # Making a copy to avoid modifying the original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Dictionaries\n",
    "    sentiment_dict = dictionaries['sentiment']\n",
    "    industry_dict = dictionaries['industry']\n",
    "    job_dict = dictionaries['job']\n",
    "    technology_dict = dictionaries['technology']\n",
    "    \n",
    "    # Detecting industries\n",
    "    print(\"Detecting industries and jobs...\")\n",
    "    df_enhanced['detected_industries'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_industries(\n",
    "            x, \n",
    "            industry_dict['industry_terms'], \n",
    "            industry_dict['industry_term_weights']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Detecting jobs\n",
    "    df_enhanced['detected_jobs'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: detect_jobs(x, job_dict['job_terms'])\n",
    "    )\n",
    "    \n",
    "    # Identifying technologies\n",
    "    print(\"Identifying AI technologies...\")\n",
    "    df_enhanced['ai_technologies'] = df_enhanced['cleaned_text'].apply(\n",
    "        lambda x: identify_technologies(\n",
    "            x, \n",
    "            technology_dict['technology_terms'],\n",
    "            technology_dict['ai_models']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extracting the organizations\n",
    "    print(\"Extracting organizations...\")\n",
    "    df_enhanced['top_organizations'] = df_enhanced['cleaned_text'].apply(extract_organizations)\n",
    "    \n",
    "    # Analyzing the sentiment\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    df_enhanced['sentiment_scores'] = df_enhanced.apply(\n",
    "        lambda x: analyze_sentiment(\n",
    "            x['cleaned_text'],\n",
    "            sentiment_dict['positive_terms'],\n",
    "            sentiment_dict['negative_terms'],\n",
    "            x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extracting sentiment scores\n",
    "    df_enhanced['sentiment_overall'] = df_enhanced['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "    df_enhanced['sentiment_base'] = df_enhanced['sentiment_scores'].apply(lambda x: x['base'])\n",
    "    df_enhanced['sentiment_lexicon'] = df_enhanced['sentiment_scores'].apply(lambda x: x['lexicon'])\n",
    "    df_enhanced['sentiment_proximity'] = df_enhanced['sentiment_scores'].apply(lambda x: x['proximity'])\n",
    "    df_enhanced['sentiment_industry'] = df_enhanced['sentiment_scores'].apply(lambda x: x['industry'])\n",
    "    \n",
    "    # Adding primary industry and job\n",
    "    df_enhanced['primary_industry'] = df_enhanced['detected_industries'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    df_enhanced['primary_job'] = df_enhanced['detected_jobs'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Runs the pipeline\n",
    "def run_enhancement_pipeline():\n",
    "    print(\"Starting enhancement pipeline...\")\n",
    "    \n",
    "    # Loading the topic data\n",
    "    df = load_from_cache('data_with_topics.pkl')\n",
    "    if df is None:\n",
    "        print(\"ERROR: Could not load data from data_with_topics.pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded data with {len(df)} articles\")\n",
    "    \n",
    "    # Creating the dictionaries\n",
    "    dictionaries = create_dictionaries()\n",
    "    \n",
    "    # Adding the features to the dataset\n",
    "    df_enhanced = add_features_to_dataset(df, dictionaries)\n",
    "    \n",
    "    # Save dataset\n",
    "    save_to_cache(df_enhanced, 'enhanced_data_with_features.pkl')\n",
    "    \n",
    "    print(\"Enhancement pipeline complete!\")\n",
    "    print(\"Enhanced data saved to 'enhanced_data_with_features.pkl'\")\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhancement pipeline...\n",
      "Loaded data with 184391 articles\n",
      "Creating sentiment dictionaries...\n",
      "Creating industry dictionaries...\n",
      "Creating job dictionaries...\n",
      "Creating technology dictionaries...\n",
      "Adding features to dataset...\n",
      "Detecting industries and jobs...\n",
      "Identifying AI technologies...\n",
      "Extracting organizations...\n",
      "Analyzing sentiment...\n",
      "Saved enhanced_data_with_features.pkl to cache\n",
      "Enhancement pipeline complete!\n",
      "Enhanced data saved to 'enhanced_data_with_features.pkl'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_enhancement_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
