{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache directory: /Users/casey/Documents/GitHub/AI_impact_employment/cache\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "cache_dir = \"cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"Using cache directory: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "def get_cache_path(filename):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def save_to_cache(obj, filename):\n",
    "    \"\"\"Save object to cache\"\"\"\n",
    "    with open(get_cache_path(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved {filename} to cache\")\n",
    "\n",
    "def load_from_cache(filename):\n",
    "    \"\"\"Load object from cache if it exists\"\"\"\n",
    "    cache_path = get_cache_path(filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Visuals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     dictionaries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimpact\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m create_impact_dictionaries()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dictionaries\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mVisuals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     create_sentiment_dictionaries,\n\u001b[1;32m     27\u001b[0m     create_industry_dictionaries,\n\u001b[1;32m     28\u001b[0m     create_job_dictionaries,\n\u001b[1;32m     29\u001b[0m     create_technology_dictionaries,\n\u001b[1;32m     30\u001b[0m     create_impact_dictionaries\n\u001b[1;32m     31\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Visuals'"
     ]
    }
   ],
   "source": [
    "# Dictionaries for analysis\n",
    "def create_dictionaries():\n",
    "    \"\"\"Create all the dictionaries needed for analysis\"\"\"\n",
    "    print(\"Creating dictionaries for analysis...\")\n",
    "    \n",
    "    dictionaries = {}\n",
    "    \n",
    "    # Sentiment Dictionaries\n",
    "    dictionaries['sentiment'] = create_sentiment_dictionaries()\n",
    "    \n",
    "    # Industry Dictionaries\n",
    "    dictionaries['industry'] = create_industry_dictionaries()\n",
    "    \n",
    "    # Job Dictionaries\n",
    "    dictionaries['job'] = create_job_dictionaries()\n",
    "    \n",
    "    # Technology Dictionaries\n",
    "    dictionaries['technology'] = create_technology_dictionaries()\n",
    "    \n",
    "    # Impact Dictionaries\n",
    "    dictionaries['impact'] = create_impact_dictionaries()\n",
    "    \n",
    "    return dictionaries\n",
    "\n",
    "from Visuals import (\n",
    "    create_sentiment_dictionaries,\n",
    "    create_industry_dictionaries,\n",
    "    create_job_dictionaries,\n",
    "    create_technology_dictionaries,\n",
    "    create_impact_dictionaries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def detect_industries(text, industry_terms, industry_term_weights=None):\n",
    "    \"\"\"\n",
    "    Detect industries mentioned in text with weighted terms\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        industry_terms: Dictionary of industries and terms\n",
    "        industry_term_weights: Optional dictionary of weights for specific terms\n",
    "        \n",
    "    Returns:\n",
    "        List of industries sorted by relevance\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Count occurrences of each category's keywords with weights\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in industry_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Apply term-specific weight if available\n",
    "                weight = 1.0\n",
    "                if industry_term_weights and category in industry_term_weights and term in industry_term_weights[category]:\n",
    "                    weight = industry_term_weights[category][term]\n",
    "                \n",
    "                # Apply additional weight for longer, more specific terms\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Calculate final score\n",
    "                score = count * weight * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the categories (without scores)\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "def detect_jobs(text, job_terms):\n",
    "    \"\"\"\n",
    "    Detect job categories mentioned in text\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        job_terms: Dictionary of job categories and terms\n",
    "        \n",
    "    Returns:\n",
    "        List of job categories sorted by relevance\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Count occurrences of each category's keywords\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in job_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                # Apply additional weight for longer, more specific terms\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                \n",
    "                # Calculate final score\n",
    "                score = count * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the categories (without scores)\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "def identify_technologies(text, technology_terms, ai_models):\n",
    "    \"\"\"\n",
    "    Identify AI technologies mentioned in the text\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        technology_terms: Dictionary of technology categories and terms\n",
    "        ai_models: List of AI model names\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of technology categories and matched terms\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    found_techs = {}\n",
    "    \n",
    "    # Technology categories\n",
    "    for tech_category, keywords in technology_terms.items():\n",
    "        matched_keywords = [k for k in keywords if k in text_lower]\n",
    "        if matched_keywords:\n",
    "            # Sort by length (longer terms are typically more specific)\n",
    "            matched_keywords.sort(key=len, reverse=True)\n",
    "            found_techs[tech_category] = matched_keywords\n",
    "    \n",
    "    # AI models\n",
    "    found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "    if found_models:\n",
    "        found_techs['specific_models'] = found_models\n",
    "    \n",
    "    return found_techs\n",
    "\n",
    "def extract_organizations(text):\n",
    "    \"\"\"\n",
    "    Extract organization names using simple heuristics\n",
    "    (Note: This is a simplified version without spaCy for faster processing)\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        \n",
    "    Returns:\n",
    "        List of potential organization names\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Known major AI companies and organizations\n",
    "    known_orgs = [\n",
    "        'OpenAI', 'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Facebook',\n",
    "        'IBM', 'Anthropic', 'Cohere', 'NVIDIA', 'Intel', 'AMD', 'Baidu', 'Alibaba',\n",
    "        'Tencent', 'Samsung', 'Tesla', 'DeepMind', 'Huawei', 'Oracle', 'SAP',\n",
    "        'Salesforce', 'Adobe', 'Cisco', 'McKinsey', 'Accenture', 'Deloitte', 'PwC',\n",
    "        'KPMG', 'MIT', 'Stanford', 'Harvard', 'Berkeley', 'Carnegie Mellon',\n",
    "        'Oxford', 'Cambridge', 'ETH Zurich', 'Max Planck', 'DeepL', 'Stability AI'\n",
    "    ]\n",
    "    \n",
    "    # Find known organizations in the text\n",
    "    found_orgs = []\n",
    "    for org in known_orgs:\n",
    "        if org.lower() in text.lower():\n",
    "            found_orgs.append(org)\n",
    "    \n",
    "    # Simple pattern for potential organizations (very simplified)\n",
    "    org_pattern = r'\\b([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*(?:\\s(?:Inc|LLC|Ltd|Corp|Corporation|Company|Technologies|AI|Labs))?)\\b'\n",
    "    \n",
    "    potential_orgs = re.findall(org_pattern, text)\n",
    "    \n",
    "    # Filter to only include potential organization names (longer than 1 word)\n",
    "    additional_orgs = [org for org in potential_orgs if len(org.split()) > 1 and org not in found_orgs]\n",
    "    \n",
    "    # Combine the results, prioritizing known organizations\n",
    "    all_orgs = found_orgs + additional_orgs[:5]  # Limit to avoid noise\n",
    "    \n",
    "    return all_orgs[:10]  # Limit to top 10\n",
    "\n",
    "def analyze_sentiment(text, positive_terms, negative_terms, industry=None):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of the article regarding AI impact with domain-specific lexicon\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        positive_terms: Dictionary of positive terms and their weights\n",
    "        negative_terms: Dictionary of negative terms and their weights\n",
    "        industry: Optional industry for industry-specific analysis\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sentiment scores\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'overall': 0,\n",
    "            'base': 0,\n",
    "            'lexicon': 0,\n",
    "            'proximity': 0,\n",
    "            'industry': 0\n",
    "        }\n",
    "    \n",
    "    # Base sentiment from TextBlob\n",
    "    base_sentiment = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    # Custom domain-specific lexicon approach\n",
    "    text_lower = text.lower()\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # 1. Calculate overall sentiment using domain-specific lexicon\n",
    "    positive_matches = 0\n",
    "    positive_score = 0\n",
    "    negative_matches = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    # Count and score positive terms\n",
    "    for term, value in positive_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            positive_matches += count\n",
    "            positive_score += value * count\n",
    "    \n",
    "    # Count and score negative terms\n",
    "    for term, value in negative_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            negative_matches += count\n",
    "            negative_score += value * count\n",
    "    \n",
    "    # 2. Calculate proximity between AI terms and impact terms\n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning']\n",
    "    impact_terms = ['job', 'work', 'employee', 'career', 'industry']\n",
    "    \n",
    "    proximity_score = 0\n",
    "    proximity_count = 0\n",
    "    \n",
    "    # Check sentences containing both AI and impact terms\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        has_ai = any(term in sentence for term in ai_terms)\n",
    "        has_impact = any(term in sentence for term in impact_terms)\n",
    "        \n",
    "        if has_ai and has_impact:\n",
    "            # Calculate sentiment for this sentence\n",
    "            sent_sentiment = TextBlob(sentence).sentiment.polarity\n",
    "            proximity_score += sent_sentiment\n",
    "            proximity_count += 1\n",
    "    \n",
    "    # 3. Industry-specific sentiment (simplified)\n",
    "    industry_sentiment = 0\n",
    "    \n",
    "    # 4. Calculate final weighted sentiment scores\n",
    "    lexicon_sentiment = 0\n",
    "    if (positive_matches + negative_matches) > 0:\n",
    "        lexicon_sentiment = (positive_score + negative_score) / (positive_matches + negative_matches)\n",
    "    \n",
    "    proximity_sentiment = 0\n",
    "    if proximity_count > 0:\n",
    "        proximity_sentiment = proximity_score / proximity_count\n",
    "    \n",
    "    # Final weighted score\n",
    "    weights = {\n",
    "        'base': 0.2,\n",
    "        'lexicon': 0.4,\n",
    "        'proximity': 0.3,\n",
    "        'industry': 0.1\n",
    "    }\n",
    "    \n",
    "    final_sentiment = (\n",
    "        weights['base'] * base_sentiment +\n",
    "        weights['lexicon'] * lexicon_sentiment +\n",
    "        weights['proximity'] * proximity_sentiment +\n",
    "        weights['industry'] * industry_sentiment\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': final_sentiment,\n",
    "        'base': base_sentiment,\n",
    "        'lexicon': lexicon_sentiment,\n",
    "        'proximity': proximity_sentiment,\n",
    "        'industry': industry_sentiment\n",
    "    }\n",
    "\n",
    "def add_features_to_dataset(df, dictionaries):\n",
    "    \"\"\"\n",
    "    Add all the missing features to the dataset\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with topic modeling results\n",
    "        dictionaries: Dictionary of analysis dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added features\n",
    "    \"\"\"\n",
    "    print(\"Adding features to dataset...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Unpack dictionaries\n",
    "    sentiment_dict = dictionaries['sentiment']\n",
    "    industry_dict = dictionaries['industry']\n",
    "    job_dict = dictionaries['job']\n",
    "    technology_dict = dictionaries['technology']\n",
    "    \n",
    "    # 1. Detect industries\n",
    "    print(\"Detecting industries and jobs...\")\n",
    "    tqdm.pandas(desc=\"Industries\")\n",
    "    df_enhanced['detected_industries'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: detect_industries(\n",
    "            x, \n",
    "            industry_dict['industry_terms'], \n",
    "            industry_dict['industry_term_weights']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2. Detect jobs\n",
    "    tqdm.pandas(desc=\"Jobs\")\n",
    "    df_enhanced['detected_jobs'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: detect_jobs(x, job_dict['job_terms'])\n",
    "    )\n",
    "    \n",
    "    # 3. Identify technologies\n",
    "    print(\"Identifying AI technologies...\")\n",
    "    tqdm.pandas(desc=\"Technologies\")\n",
    "    df_enhanced['ai_technologies'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: identify_technologies(\n",
    "            x, \n",
    "            technology_dict['technology_terms'],\n",
    "            technology_dict['ai_models']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 4. Extract organizations (simplified version)\n",
    "    print(\"Extracting organizations...\")\n",
    "    tqdm.pandas(desc=\"Organizations\")\n",
    "    df_enhanced['top_organizations'] = df_enhanced['cleaned_text'].progress_apply(extract_organizations)\n",
    "    \n",
    "    # 5. Analyze sentiment\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    tqdm.pandas(desc=\"Sentiment\")\n",
    "    df_enhanced['sentiment_scores'] = df_enhanced.progress_apply(\n",
    "        lambda x: analyze_sentiment(\n",
    "            x['cleaned_text'],\n",
    "            sentiment_dict['positive_terms'],\n",
    "            sentiment_dict['negative_terms'],\n",
    "            x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract components of sentiment\n",
    "    df_enhanced['sentiment_overall'] = df_enhanced['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "    df_enhanced['sentiment_base'] = df_enhanced['sentiment_scores'].apply(lambda x: x['base'])\n",
    "    df_enhanced['sentiment_lexicon'] = df_enhanced['sentiment_scores'].apply(lambda x: x['lexicon'])\n",
    "    df_enhanced['sentiment_proximity'] = df_enhanced['sentiment_scores'].apply(lambda x: x['proximity'])\n",
    "    df_enhanced['sentiment_industry'] = df_enhanced['sentiment_scores'].apply(lambda x: x['industry'])\n",
    "    \n",
    "    # 6. Add primary industry and job\n",
    "    df_enhanced['primary_industry'] = df_enhanced['detected_industries'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    df_enhanced['primary_job'] = df_enhanced['detected_jobs'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "def run_enhancement_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to run the enhancement pipeline\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Starting enhancement pipeline...\")\n",
    "    \n",
    "    # 1. Load topic data\n",
    "    df = load_from_cache('data_with_topics.pkl')\n",
    "    if df is None:\n",
    "        print(\"ERROR: Could not load data from data_with_topics.pkl\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded data with {len(df)} articles\")\n",
    "    \n",
    "    # 2. Create dictionaries\n",
    "    dictionaries = create_dictionaries()\n",
    "    \n",
    "    # 3. Add features to dataset\n",
    "    df_enhanced = add_features_to_dataset(df, dictionaries)\n",
    "    \n",
    "    # 4. Save enhanced dataset\n",
    "    save_to_cache(df_enhanced, 'enhanced_data_with_features.pkl')\n",
    "    \n",
    "    print(\"Enhancement pipeline complete!\")\n",
    "    print(\"Enhanced data saved to 'enhanced_data_with_features.pkl'\")\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhancement pipeline...\n",
      "Loaded data with 184391 articles\n",
      "Creating dictionaries for analysis...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_sentiment_dictionaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     run_enhancement_pipeline()\n",
      "Cell \u001b[0;32mIn[5], line 364\u001b[0m, in \u001b[0;36mrun_enhancement_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# 2. Create dictionaries\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m dictionaries \u001b[38;5;241m=\u001b[39m create_dictionaries()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# 3. Add features to dataset\u001b[39;00m\n\u001b[1;32m    367\u001b[0m df_enhanced \u001b[38;5;241m=\u001b[39m add_features_to_dataset(df, dictionaries)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mcreate_dictionaries\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m dictionaries \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Sentiment Dictionaries\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dictionaries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m create_sentiment_dictionaries()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Industry Dictionaries\u001b[39;00m\n\u001b[1;32m     12\u001b[0m dictionaries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m create_industry_dictionaries()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_sentiment_dictionaries' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_enhancement_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
