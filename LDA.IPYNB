{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling script for AI impact on employment analysis\n",
    "# Set random seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory if it doesn't exist\n",
    "cache_dir = \"cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "def get_cache_path(filename):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def save_to_cache(obj, filename):\n",
    "    \"\"\"Save object to cache\"\"\"\n",
    "    with open(get_cache_path(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_from_cache(filename):\n",
    "    \"\"\"Load object from cache if it exists\"\"\"\n",
    "    cache_path = get_cache_path(filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(df_input, num_topics=10, force_recompute=False):\n",
    "    \"\"\"\n",
    "    Run topic modeling on articles\n",
    "    \n",
    "    Args:\n",
    "        df_input: DataFrame with cleaned articles (must contain 'cleaned_text' column)\n",
    "        num_topics: Number of topics to extract\n",
    "        force_recompute: Whether to force recomputation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (topic_model, document_topics, feature_names)\n",
    "    \"\"\"\n",
    "    if df_input is None:\n",
    "        print(\"ERROR: Input DataFrame is None!\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    if 'cleaned_text' not in df_input.columns:\n",
    "        print(\"ERROR: Input DataFrame has no 'cleaned_text' column!\")\n",
    "        print(f\"Available columns: {df_input.columns.tolist()}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    cache_file = f\"topic_model_{num_topics}.pkl\"\n",
    "    \n",
    "    # Results\n",
    "    if not force_recompute:\n",
    "        cached_data = load_from_cache(cache_file)\n",
    "        if cached_data is not None:\n",
    "            print(\"Loaded topic model from cache\")\n",
    "            return cached_data\n",
    "    \n",
    "    print(\"Running topic modeling...\")\n",
    "    \n",
    "    documents = df_input['cleaned_text'].tolist()\n",
    "    if not documents:\n",
    "        print(\"ERROR: No documents found in the cleaned_text column!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Use CountVectorizer with n-grams\n",
    "    print(\"Creating document-term matrix...\")\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.7,\n",
    "        min_df=10,\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    try:\n",
    "        X = vectorizer.fit_transform(documents)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in vectorization: {e}\")\n",
    "        # Return placeholder if we encounter an error\n",
    "        return None, np.array([0] * len(documents)), [], []\n",
    "    \n",
    "    # Train LDA model\n",
    "    print(f\"Training LDA model with {num_topics} topics...\")\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        max_iter=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit model and transform documents\n",
    "    try:\n",
    "        doc_topic_dists = lda.fit_transform(X)\n",
    "        \n",
    "        # Get the most probable topic for each document\n",
    "        doc_topics = doc_topic_dists.argmax(axis=1)\n",
    "        \n",
    "        # Extract top terms for each topic\n",
    "        topic_terms = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_features_ind = topic.argsort()[:-10 - 1:-1]\n",
    "            top_features = [feature_names[i] for i in top_features_ind]\n",
    "            topic_terms.append(top_features)\n",
    "        \n",
    "        # Save topic model, document topics, and feature names\n",
    "        result = (lda, doc_topics, feature_names, topic_terms)\n",
    "        save_to_cache(result, cache_file)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in topic modeling: {e}\")\n",
    "        return None, np.array([0] * len(documents)), [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics(lda, feature_names, n_top_words=10, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the top words for each topic in the LDA model\n",
    "    \n",
    "    Args:\n",
    "        lda: Trained LDA model\n",
    "        feature_names: Names of features from vectorizer\n",
    "        n_top_words: Number of top words to show per topic\n",
    "        save_path: Path to save the figure, if None just displays\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(15, 25), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        if topic_idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        \n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.set_xlabel('Weight', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics_over_time(df, topics, topic_terms, top_n_topics=5, save_path=None):\n",
    "    \"\"\"Plot the prevalence of top topics over time\"\"\"\n",
    "    # Create DataFrame with topics\n",
    "    topic_df = pd.DataFrame({'date': df['date'], 'topic': topics})\n",
    "    \n",
    "    # Counts of each topic\n",
    "    topic_counts = {}\n",
    "    for topic in range(max(topics) + 1):\n",
    "        topic_counts[topic] = np.sum(topics == topic)\n",
    "    \n",
    "    # Get top topics\n",
    "    top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_topics = [topic for topic, count in top_topics[:top_n_topics]]\n",
    "    \n",
    "    # Top topics by month\n",
    "    topic_df = topic_df[topic_df['topic'].isin(top_topics)]\n",
    "    topic_df['yearmonth'] = topic_df['date'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Topics per month\n",
    "    topic_time = topic_df.groupby(['yearmonth', 'topic']).size().reset_index(name='count')\n",
    "    \n",
    "    # Datetime\n",
    "    topic_time['date'] = pd.to_datetime(topic_time['yearmonth'] + '-01')\n",
    "    topic_time = topic_time.sort_values('date')\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot_df = topic_time.pivot(index='date', columns='topic', values='count').fillna(0)\n",
    "    \n",
    "    # Add topic labels from terms\n",
    "    topic_labels = {}\n",
    "    for topic in top_topics:\n",
    "        # Label from top 3 words\n",
    "        label = ', '.join(topic_terms[topic][:3])\n",
    "        topic_labels[topic] = f\"Topic {topic}: {label}\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if topic_labels:\n",
    "        pivot_df = pivot_df.rename(columns=topic_labels)\n",
    "    \n",
    "    pivot_df.plot(kind='line', figsize=(12, 6))\n",
    "    plt.title('Top Topics Over Time', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Article Count', fontsize=14)\n",
    "    plt.legend(title='Topic', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Neither clean_filter.py nor clean_and_filter.py found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     file_to_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_and_filter.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeither clean_filter.py nor clean_and_filter.py found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Then use the file that exists\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Neither clean_filter.py nor clean_and_filter.py found"
     ]
    }
   ],
   "source": [
    "# First, check which file exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('clean_filter.py'):\n",
    "    file_to_use = 'clean_filter.py'\n",
    "elif os.path.exists('clean_and_filter.py'):\n",
    "    file_to_use = 'clean_and_filter.py'\n",
    "else:\n",
    "    raise FileNotFoundError(\"Neither clean_filter.py nor clean_and_filter.py found\")\n",
    "\n",
    "# Then use the file that exists\n",
    "try:\n",
    "    exec(open(file_to_use).read())\n",
    "    print(f\"Successfully executed {file_to_use}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing {file_to_use}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory: ['LDA.IPYNB', '.DS_Store', 'cache', 'Final_NLP_v2 (4).ipynb', 'clean_filter.py', '.git', 'Final_NLP_v2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Files in current directory:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/casey/Documents/GitHub/AI_impact_employment\n",
      "Script path: /Users/casey/Documents/GitHub/AI_impact_employment/clean_filter.py\n",
      "Script exists: True\n",
      "Starting data preprocessing...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (200083, 5)\n",
      "Cleaning and processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/casey/Documents/GitHub/AI_impact_employment/clean_filter.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing rows with invalid dates: 200083 rows\n",
      "Filtering for relevance...\n",
      "After filtering for relevance: 184391 rows\n",
      "Saving data to cache...\n",
      "Saved cleaned_data.pkl to cache\n",
      "Saved cleaned_data_for_lda.pkl to cache\n",
      "Saved cleaned_data_minimal.pkl to cache\n",
      "Data preprocessing complete!\n",
      "Processed 200083 articles, with 184391 relevant articles saved to cache\n",
      "You can now run LDA.py for topic modeling\n",
      "Successfully imported clean_filter module\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Print current working directory to verify\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Make sure the script is in the Python path\n",
    "script_path = os.path.abspath('clean_filter.py')\n",
    "print(\"Script path:\", script_path)\n",
    "print(\"Script exists:\", os.path.exists(script_path))\n",
    "\n",
    "# Import the module instead of using exec\n",
    "if os.path.exists('clean_filter.py'):\n",
    "    try:\n",
    "        import clean_filter\n",
    "        print(\"Successfully imported clean_filter module\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing clean_filter: {e}\")\n",
    "else:\n",
    "    print(\"clean_filter.py not found in specified path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load cleaned_data_for_lda.pkl...\n",
      "Error loading cleaned_data_for_lda.pkl: name 'load_from_cache' is not defined\n",
      "Attempting to load cleaned_data_minimal.pkl...\n",
      "Error loading cleaned_data_minimal.pkl: name 'load_from_cache' is not defined\n",
      "Attempting to load cleaned_data.pkl...\n",
      "Error loading cleaned_data.pkl: name 'load_from_cache' is not defined\n",
      "ERROR: Could not load valid data from any cache file!\n",
      "Please run clean_and_filter.py first to generate the necessary data files.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Verify we have the necessary data\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_clean)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run topic modeling\u001b[39;00m\n\u001b[1;32m     34\u001b[0m lda, doc_topics, feature_names, topic_terms \u001b[38;5;241m=\u001b[39m run_topic_modeling(\n\u001b[1;32m     35\u001b[0m     df_clean, \n\u001b[1;32m     36\u001b[0m     num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     37\u001b[0m     force_recompute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     38\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Try loading from each of the possible cache files\n",
    "    df_clean = None\n",
    "    cache_files = [\"cleaned_data_for_lda.pkl\", \"cleaned_data_minimal.pkl\", \"cleaned_data.pkl\"]\n",
    "    \n",
    "    for cache_file in cache_files:\n",
    "        print(f\"Attempting to load {cache_file}...\")\n",
    "        try:\n",
    "            df_clean = load_from_cache(cache_file)\n",
    "            if df_clean is not None and 'cleaned_text' in df_clean.columns:\n",
    "                print(f\"Successfully loaded data from {cache_file}\")\n",
    "                print(f\"DataFrame shape: {df_clean.shape}\")\n",
    "                print(f\"Columns: {df_clean.columns.tolist()}\")\n",
    "                break\n",
    "            else:\n",
    "                if df_clean is None:\n",
    "                    print(f\"File {cache_file} not found or could not be loaded\")\n",
    "                else:\n",
    "                    print(f\"File {cache_file} loaded but 'cleaned_text' column is missing\")\n",
    "                df_clean = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {cache_file}: {e}\")\n",
    "            df_clean = None\n",
    "    \n",
    "    if df_clean is None:\n",
    "        print(\"ERROR: Could not load valid data from any cache file!\")\n",
    "        print(\"Please run clean_and_filter.py first to generate the necessary data files.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Verify we have the necessary data\n",
    "    print(f\"Data loaded with {len(df_clean)} articles\")\n",
    "    \n",
    "    # Run topic modeling\n",
    "    lda, doc_topics, feature_names, topic_terms = run_topic_modeling(\n",
    "        df_clean, \n",
    "        num_topics=10, \n",
    "        force_recompute=True\n",
    "    )\n",
    "    \n",
    "    if lda is None:\n",
    "        print(\"ERROR: Topic modeling failed!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Add topic to each document\n",
    "    df_clean['topic'] = doc_topics\n",
    "    \n",
    "    print(\"Creating topic visualizations...\")\n",
    "    \n",
    "    # Plot top words for each topic\n",
    "    fig1 = plot_topics(lda, feature_names, n_top_words=10, save_path=\"topic_words.png\")\n",
    "    \n",
    "    # Plot topics over time\n",
    "    fig2 = plot_topics_over_time(df_clean, doc_topics, topic_terms, top_n_topics=5, save_path=\"topics_over_time.png\")\n",
    "    \n",
    "    # Save the enriched DataFrame with topics\n",
    "    save_to_cache(df_clean, \"data_with_topics.pkl\")\n",
    "    \n",
    "    print(\"Topic modeling and visualization complete!\")\n",
    "    print(\"Saved topic visualization to 'topic_words.png'\")\n",
    "    print(\"Saved topics over time to 'topics_over_time.png'\")\n",
    "    print(\"Saved enriched data to cache as 'data_with_topics.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
