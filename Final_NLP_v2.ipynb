{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
      "aeon 1.0.0 requires numpy<2.1.0,>=1.21.0, but you have numpy 2.2.5 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n",
      "sktime 0.35.0 requires numpy<2.2,>=1.21, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4g/7cwxt52n09sb7vzyh6tzj3w00000gn/T/ipykernel_23790/2220291325.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4g/7cwxt52n09sb7vzyh6tzj3w00000gn/T/ipykernel_23790/4262355253.py\", line 2, in <module>\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 11, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 8, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 11, in <module>\n",
      "    from scipy.sparse import csr_matrix, issparse\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ML & NLP\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer, CountVectorizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation, NMF\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/__init__.py:295\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/_csr.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[1;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# ML & NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding & Preparation\n",
    "\n",
    "1. I will load and explore the dataset structure.\n",
    "2. Then I will examine the articles content, sources, publication dates, and metadata.\n",
    "3. I will clean the data by removing web crawl artifacts (HTML tags, etc).\n",
    "4. I will filter out irrelevant articles that don't focus on AI's impact on industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4g/7cwxt52n09sb7vzyh6tzj3w00000gn/T/ipykernel_23790/3883119864.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://businessnewsthisweek.com/business/infog...</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Infogain AI Business Solutions Now Available i...</td>\n",
       "      <td>\\n\\nInfogain AI Business Solutions Now Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://allafrica.com/stories/202504250184.html</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Africa: AI Policies in Africa - Lessons From G...</td>\n",
       "      <td>\\nAfrica: AI Policies in Africa - Lessons From...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://asiatimes.com/2023/07/yang-lan-intervi...</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Yang Lan interviews academics on AI developmen...</td>\n",
       "      <td>\\nYang Lan interviews academics on AI developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.meritalk.com/articles/commerce-nom...</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>en</td>\n",
       "      <td>Commerce Nominee Promises Increased Domestic A...</td>\n",
       "      <td>\\nCommerce Nominee Promises Increased Domestic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://citylife.capetown/hmn/uncategorized/re...</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry: Th...</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language  \\\n",
       "0  http://businessnewsthisweek.com/business/infog...  2023-05-20       en   \n",
       "1    https://allafrica.com/stories/202504250184.html  2025-04-25       en   \n",
       "2  https://asiatimes.com/2023/07/yang-lan-intervi...  2023-07-25       en   \n",
       "3  https://cdn.meritalk.com/articles/commerce-nom...  2025-02-04       en   \n",
       "4  https://citylife.capetown/hmn/uncategorized/re...  2023-11-11       en   \n",
       "\n",
       "                                               title  \\\n",
       "0  Infogain AI Business Solutions Now Available i...   \n",
       "1  Africa: AI Policies in Africa - Lessons From G...   \n",
       "2  Yang Lan interviews academics on AI developmen...   \n",
       "3  Commerce Nominee Promises Increased Domestic A...   \n",
       "4  Revolutionizing the Manufacturing Industry: Th...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nInfogain AI Business Solutions Now Availab...  \n",
       "1  \\nAfrica: AI Policies in Africa - Lessons From...  \n",
       "2  \\nYang Lan interviews academics on AI developm...  \n",
       "3  \\nCommerce Nominee Promises Increased Domestic...  \n",
       "4     Revolutionizing the Manufacturing Industry:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AINewsAnalyzer:\n",
    "    \"\"\"Main class for analyzing AI-related news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, cache_dir: str = \"cache\"):\n",
    "        \"\"\"Initialize the analyzer with data path and cache directory\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        \n",
    "        # SpaCy\n",
    "        print(\"Loading SpaCy model...\")\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        # Dictionaries for sentiment analysis\n",
    "        self._create_sentiment_dictionaries()\n",
    "        \n",
    "        # Data\n",
    "        self.df = self._load_data()\n",
    "        print(f\"Loaded dataset with {len(self.df)} articles\")\n",
    "    \n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the dataset from parquet file\"\"\"\n",
    "        return pd.read_parquet(self.data_path, engine='pyarrow')\n",
    "    \n",
    "    def _get_cache_path(self, filename: str) -> str:\n",
    "        \"\"\"Get full path for a cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, filename)\n",
    "    \n",
    "    def _save_to_cache(self, obj: Any, filename: str) -> None:\n",
    "        \"\"\"Save object to cache\"\"\"\n",
    "        with open(self._get_cache_path(filename), 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "    \n",
    "    def _load_from_cache(self, filename: str) -> Any:\n",
    "        \"\"\"Load object from cache if it exists\"\"\"\n",
    "        cache_path = self._get_cache_path(filename)\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    def _create_sentiment_dictionaries(self) -> None:\n",
    "        \"\"\"Create dictionaries for domain-specific sentiment analysis\"\"\"\n",
    "        # Positive terms related to AI\n",
    "        self.positive_terms = {\n",
    "            'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7, \n",
    "            'assist': 0.6, 'empower': 0.9, 'efficiency': 0.8, 'productivity': 0.8, \n",
    "            'innovation': 0.9, 'growth': 0.7, 'advancement': 0.8, 'collaborate': 0.7, \n",
    "            'partnership': 0.6, 'upskill': 0.9, 'complement': 0.7, 'benefit': 0.8,\n",
    "            'progress': 0.7, 'create': 0.6, 'advantage': 0.7, 'potential': 0.5,\n",
    "            'solution': 0.6, 'revolutionize': 0.8\n",
    "        }\n",
    "        \n",
    "        # Negative terms related to AI\n",
    "        self.negative_terms = {\n",
    "            'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'threaten': -0.7, \n",
    "            'risk': -0.6, 'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, \n",
    "            'downsizing': -0.8, 'automation': -0.5, 'obsolete': -0.8, 'disruption': -0.6, \n",
    "            'inequality': -0.7, 'bias': -0.7, 'surveillance': -0.8, 'danger': -0.7,\n",
    "            'concern': -0.5, 'worry': -0.6, 'fear': -0.7, 'threat': -0.8,\n",
    "            'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(self, \n",
    "                              force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main function to clean and filter the dataset\n",
    "        \n",
    "        Args:\n",
    "            force_recompute: Whether to force recomputation even if cached results exist\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cleaned and filtered data\n",
    "        \"\"\"\n",
    "        cache_file = \"cleaned_data.pkl\"\n",
    "        \n",
    "        if not force_recompute:\n",
    "            df_clean = self._load_from_cache(cache_file)\n",
    "            if df_clean is not None:\n",
    "                print(\"Loaded cleaned data from cache\")\n",
    "                return df_clean\n",
    "        \n",
    "        print(\"Cleaning and filtering data...\")\n",
    "        \n",
    "        # Clean text\n",
    "        self.df['cleaned_text'] = self.df['text'].apply(self._clean_article)\n",
    "        \n",
    "        # Parsing dates\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=['date'])\n",
    "        \n",
    "        # Time features\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        self.df['yearmonth'] = self.df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Relevance\n",
    "        self.df['is_relevant'] = self.df['cleaned_text'].apply(self._is_relevant)\n",
    "        df_relevant = self.df[self.df['is_relevant']].copy()\n",
    "        \n",
    "        # Extract for source analysis\n",
    "        df_relevant['source_domain'] = df_relevant['url'].apply(self._extract_domain)\n",
    "        \n",
    "        self._save_to_cache(df_relevant, cache_file)\n",
    "        \n",
    "        print(f\"Filtered to {len(df_relevant)} relevant articles\")\n",
    "        return df_relevant\n",
    "    \n",
    "def _clean_article(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean article text by removing HTML, extra whitespace, etc.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw article text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Handling none or empty strings\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Removing the HTML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        \n",
    "        # Removing the URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Removing the extra whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Removing the special characters\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:\\'\\\"()-]', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "def _extract_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        if not url or pd.isna(url):\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Domain using regex\n",
    "            domain_match = re.search(r'https?://(?:www\\.)?([^/]+)', url)\n",
    "            if domain_match:\n",
    "                return domain_match.group(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "def _is_relevant(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if article is relevant to AI's impact on industries/jobs\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned article text\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating relevance\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Checking for AI related terms\n",
    "        ai_terms = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', \n",
    "                   'neural network', 'llm', 'large language model', 'chatgpt', 'generative ai']\n",
    "        \n",
    "        # Checking for industry impact terms\n",
    "        impact_terms = ['impact', 'effect', 'transform', 'disrupt', 'replace', 'automate',\n",
    "                       'job', 'employment', 'workforce', 'career', 'industry', 'sector', \n",
    "                       'profession', 'work', 'labor market', 'skill']\n",
    "        \n",
    "        contains_ai = any(term in text_lower for term in ai_terms)\n",
    "        contains_impact = any(term in text_lower for term in impact_terms)\n",
    "        \n",
    "        if not (contains_ai and contains_impact):\n",
    "            return False\n",
    "        \n",
    "        #Proximity within same paragraph for better accuracy\n",
    "        paragraphs = text_lower.split('\\n')\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_has_ai = any(term in para for term in ai_terms)\n",
    "            para_has_impact = any(term in para for term in impact_terms)\n",
    "            \n",
    "            if para_has_ai and para_has_impact:\n",
    "                return True\n",
    "        \n",
    "        # Fallback for short texts without paragraphs, checking the  sentence proximity\n",
    "        sentences = text_lower.split('.')\n",
    "        \n",
    "        ai_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in ai_terms)]\n",
    "        impact_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in impact_terms)]\n",
    "        \n",
    "        # AI and impact sentences are close to each other within 3 sentences\n",
    "        for ai_idx in ai_sentences:\n",
    "            for impact_idx in impact_sentences:\n",
    "                if abs(ai_idx - impact_idx) <= 3:\n",
    "                    return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing the articles with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory for the \n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing with SpaCy\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Adding the entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separate columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Count frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry and Job Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for the entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory\n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separatating columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_technologies(self, \n",
    "                             df: pd.DataFrame,\n",
    "                             force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identify AI technologies mentioned in articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with identified technologies\n",
    "        \"\"\"\n",
    "        cache_file = \"tech_data.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_tech = self._load_from_cache(cache_file)\n",
    "            if df_tech is not None:\n",
    "                print(\"Loaded technology data from cache\")\n",
    "                return df_tech\n",
    "        \n",
    "        print(\"Identifying AI technologies...\")\n",
    "        \n",
    "        # Technology categories and it's keywords\n",
    "        technologies = {\n",
    "            'machine_learning': [\n",
    "                'machine learning', 'ml', 'supervised learning', 'unsupervised learning', \n",
    "                'reinforcement learning', 'decision trees', 'random forests', 'svm'\n",
    "            ],\n",
    "            'deep_learning': [\n",
    "                'deep learning', 'neural networks', 'cnn', 'rnn', 'lstm', 'transformer models',\n",
    "                'generative ai', 'gan', 'diffusion models'\n",
    "            ],\n",
    "            'nlp': [\n",
    "                'natural language processing', 'nlp', 'language models', 'llm', 'chatbots',\n",
    "                'sentiment analysis', 'named entity recognition', 'text classification'\n",
    "            ],\n",
    "            'computer_vision': [\n",
    "                'computer vision', 'image recognition', 'object detection', 'facial recognition',\n",
    "                'image segmentation', 'video analysis'\n",
    "            ],\n",
    "            'robotics': [\n",
    "                'robotics', 'robots', 'automation', 'robotic process automation', 'rpa',\n",
    "                'autonomous systems', 'drones', 'self-driving'\n",
    "            ],\n",
    "            'voice_ai': [\n",
    "                'voice assistant', 'speech recognition', 'text to speech', 'voice synthesis',\n",
    "                'voice computing', 'speech to text'\n",
    "            ],\n",
    "            'ai_infrastructure': [\n",
    "                'gpu', 'tpu', 'cloud computing', 'ai chips', 'neural processors',\n",
    "                'edge ai', 'ai hardware', 'quantum computing'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Model names\n",
    "        ai_models = [\n",
    "            'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'llama', 'gemini', 'claude', 'bert', \n",
    "            'stable diffusion', 'dall-e', 'midjourney', 'bard', 'palm', 'chinchilla'\n",
    "        ]\n",
    "        \n",
    "        # Technology identification\n",
    "        df['ai_technologies'] = df['cleaned_text'].apply(\n",
    "            lambda x: self._identify_technologies(x, technologies, ai_models)\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(df, cache_file)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def _identify_technologies(self, \n",
    "                              text: str, \n",
    "                              technologies: Dict[str, List[str]], \n",
    "                              ai_models: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Identify AI technologies mentioned in the text\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        found_techs = {}\n",
    "        # Technology categories\n",
    "        for tech_category, keywords in technologies.items():\n",
    "            matched_keywords = [k for k in keywords if k in text_lower]\n",
    "            if matched_keywords:\n",
    "                found_techs[tech_category] = matched_keywords\n",
    "        \n",
    "        # AI models\n",
    "        found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "        if found_models:\n",
    "            found_techs['specific_models'] = found_models\n",
    "        \n",
    "        return found_techs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 10000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyze sentiment of articles regarding AI impact\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with sentiment analysis\n",
    "        \"\"\"\n",
    "        cache_file = f\"sentiment_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_sentiment = self._load_from_cache(cache_file)\n",
    "            if df_sentiment is not None:\n",
    "                print(f\"Loaded sentiment data for {len(df_sentiment)} articles from cache\")\n",
    "                return df_sentiment\n",
    "        \n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for sentiment analysis\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Analyzing sentiment...\")\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sample_df['sentiment_scores'] = sample_df.apply(\n",
    "            lambda x: self._analyze_contextual_sentiment(\n",
    "                x['cleaned_text'], \n",
    "                x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "            ), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Extracting the components of sentiment\n",
    "        sample_df['sentiment_overall'] = sample_df['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "        sample_df['sentiment_ai_impact'] = sample_df['sentiment_scores'].apply(lambda x: x['ai_impact'])\n",
    "        sample_df['sentiment_industry'] = sample_df['sentiment_scores'].apply(lambda x: x['industry_context'])\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _analyze_contextual_sentiment(self, \n",
    "                                     text: str, \n",
    "                                     industry: Optional[str] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze sentiment with context awareness for AI impact\n",
    "        \n",
    "        Args:\n",
    "            text: Article text\n",
    "            industry: Specific industry to contextualize sentiment\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with different sentiment scores\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {'overall': 0, 'ai_impact': 0, 'industry_context': 0}\n",
    "        \n",
    "        # Base sentiment with TextBlob\n",
    "        base_sentiment = TextBlob(text).sentiment.polarity\n",
    "        \n",
    "        # AI impact sentiment using my custom dictionaries\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Occurrences and getting the weighted sentiment for domain-specific terms\n",
    "        positive_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.positive_terms.items()\n",
    "        )\n",
    "        \n",
    "        negative_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.negative_terms.items()\n",
    "        )\n",
    "        \n",
    "        # Domain-specific sentiment\n",
    "        ai_impact_mentions = sum(text_lower.count(term) for term in self.positive_terms) + \\\n",
    "                             sum(text_lower.count(term) for term in self.negative_terms)\n",
    "        \n",
    "        if ai_impact_mentions > 0:\n",
    "            ai_impact_sentiment = (positive_sentiment + negative_sentiment) / ai_impact_mentions\n",
    "        else:\n",
    "            ai_impact_sentiment = 0\n",
    "        \n",
    "        # Industry context sentiment\n",
    "        industry_context_sentiment = 0\n",
    "        if industry:\n",
    "            # Industry-specific terms\n",
    "            industry_terms = []\n",
    "            \n",
    "            if industry == 'healthcare':\n",
    "                industry_terms = ['patient', 'doctor', 'nurse', 'hospital', 'medical', 'diagnosis', 'treatment']\n",
    "            elif industry == 'finance':\n",
    "                industry_terms = ['bank', 'investment', 'trading', 'financial', 'insurance', 'loan']\n",
    "            elif industry == 'manufacturing':\n",
    "                industry_terms = ['factory', 'production', 'assembly', 'industrial', 'manufacturing']\n",
    "            elif industry == 'technology':\n",
    "                industry_terms = ['software', 'hardware', 'startup', 'tech', 'computing', 'digital']\n",
    "            elif industry == 'education':\n",
    "                industry_terms = ['student', 'teacher', 'school', 'learning', 'education', 'university']\n",
    "            \n",
    "            if industry_terms:\n",
    "                # Finding sentences containing industry terms\n",
    "                sentences = text.split('.')\n",
    "                industry_sentences = [s for s in sentences if any(term in s.lower() for term in industry_terms)]\n",
    "                \n",
    "                if industry_sentences:\n",
    "                    # Sentiment for industry-specific sentences\n",
    "                    industry_sentiment = np.mean([TextBlob(s).sentiment.polarity for s in industry_sentences])\n",
    "                    industry_context_sentiment = industry_sentiment\n",
    "        \n",
    "        # Sentiment is a weighted average of the base sentiment and AI impact sentiment\n",
    "        overall_sentiment = 0.4 * base_sentiment + 0.6 * ai_impact_sentiment\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_sentiment,\n",
    "            'ai_impact': ai_impact_sentiment,\n",
    "            'industry_context': industry_context_sentiment\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(self, \n",
    "                          df: pd.DataFrame,\n",
    "                          num_topics: int = 10,\n",
    "                          force_recompute: bool = False) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run topic modeling on articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            num_topics: Number of topics to extract\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (topic_model, document_topics)\n",
    "        \"\"\"\n",
    "        cache_file = f\"topic_model_{num_topics}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            cached_data = self._load_from_cache(cache_file)\n",
    "            if cached_data is not None:\n",
    "                print(\"Loaded topic model from cache\")\n",
    "                return cached_data\n",
    "        \n",
    "        print(\"Running topic modeling...\")\n",
    "        \n",
    "        documents = df['cleaned_text'].tolist()\n",
    "        \n",
    "        # Embedding\n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Vectorizer with bigrams and trigrams\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words=\"english\", \n",
    "            ngram_range=(1, 3),\n",
    "            min_df=5,\n",
    "            max_df=0.7\n",
    "        )\n",
    "        \n",
    "        # BERTopic model\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            nr_topics=num_topics\n",
    "        )\n",
    "        \n",
    "        topics, probs = topic_model.fit_transform(documents)\n",
    "        \n",
    "        # Topic representations\n",
    "        topic_model.update_topics(documents, topics, n_gram_range=(1, 3))\n",
    "        \n",
    "        self._save_to_cache((topic_model, topics), cache_file)\n",
    "        \n",
    "        return topic_model, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(self, df: pd.DataFrame) -> Dict[str, plt.Figure]:\n",
    "        figures = {}\n",
    "        \n",
    "        # Industry Impact Heat Map\n",
    "        figures['industry_heatmap'] = self._create_industry_impact_heatmap(df)\n",
    "        \n",
    "        # Sentiment Timeline\n",
    "        figures['sentiment_timeline'] = self._create_sentiment_timeline(df)\n",
    "        \n",
    "        # Technology Adoption Timeline\n",
    "        figures['technology_timeline'] = self._create_technology_timeline(df)\n",
    "        \n",
    "        # AI organization Strategy Comparison\n",
    "        if 'top_organizations' in df.columns and 'ai_technologies' in df.columns:\n",
    "            figures['org_strategy'] = self._create_organization_ai_strategy_comparison(df)\n",
    "        \n",
    "        # Job Impact Network\n",
    "        figures['job_impact'] = self._create_tech_job_impact_network(df)\n",
    "        \n",
    "        # Topic Distribution\n",
    "        if hasattr(self, 'topics') and len(self.topics) == len(df):\n",
    "            figures['topic_timeline'] = self._plot_topics_over_time(df, self.topics)\n",
    "        \n",
    "        return figures\n",
    "    \n",
    "def _create_industry_impact_heatmap(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create a heatmap showing AI's impact on different industries based on sentiment\"\"\"\n",
    "        # Creating one row per industry\n",
    "        exploded_df = df.explode('detected_industries').dropna(subset=['detected_industries'])\n",
    "        \n",
    "        # Sentiment scores\n",
    "        if 'sentiment_ai_impact' in exploded_df.columns:\n",
    "            exploded_df['sentiment_score'] = exploded_df['sentiment_ai_impact']\n",
    "        else:\n",
    "            exploded_df['sentiment_score'] = exploded_df['sentiment_overall']\n",
    "        \n",
    "        # Articles per industry\n",
    "        industry_counts = exploded_df['detected_industries'].value_counts()\n",
    "        \n",
    "        # Top industries\n",
    "        top_industries = industry_counts.head(10).index.tolist()\n",
    "        \n",
    "        # Top industries\n",
    "        industry_df = exploded_df[exploded_df['detected_industries'].isin(top_industries)]\n",
    "        \n",
    "        # Grouping by industry and year\n",
    "        industry_df['year'] = industry_df['date'].dt.year\n",
    "        industry_year_sentiment = industry_df.groupby(['detected_industries', 'year'])['sentiment_score'].mean().reset_index()\n",
    "        \n",
    "        pivot_df = industry_year_sentiment.pivot(index='detected_industries', columns='year', values='sentiment_score')\n",
    "        \n",
    "        #  Heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(pivot_df, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
    "        plt.title('AI Impact Sentiment by Industry and Year')\n",
    "        plt.ylabel('Industry')\n",
    "        plt.xlabel('Year')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_sentiment_timeline(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create timeline of sentiment with key events\"\"\"\n",
    "        # Average sentiment by month\n",
    "        df['yearmonth'] = df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Sentiment scores\n",
    "        if 'sentiment_ai_impact' in df.columns:\n",
    "            sentiment_col = 'sentiment_ai_impact'\n",
    "        elif 'sentiment_overall' in df.columns:\n",
    "            sentiment_col = 'sentiment_overall'\n",
    "        else:\n",
    "            sentiment_col = 'sentiment' if 'sentiment' in df.columns else None\n",
    "        \n",
    "        if not sentiment_col:\n",
    "            print(\"No sentiment column found for timeline\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        # By month\n",
    "        monthly_sentiment = df.groupby('yearmonth')[sentiment_col].mean().reset_index()\n",
    "        \n",
    "        # Datetime format\n",
    "        monthly_sentiment['date'] = pd.to_datetime(monthly_sentiment['yearmonth'] + '-01')\n",
    "        monthly_sentiment = monthly_sentiment.sort_values('date')\n",
    "        \n",
    "        # Key AI events to annotate\n",
    "        key_events = [\n",
    "            {'date': '2023-11', 'event': 'GPT-4 Release', 'y_pos': 0.1},\n",
    "            {'date': '2024-03', 'event': 'AI Regulation Act', 'y_pos': -0.1},\n",
    "            {'date': '2024-06', 'event': 'Major AI Job Study', 'y_pos': 0.2},\n",
    "            {'date': '2024-09', 'event': 'New NLP Breakthrough', 'y_pos': -0.2}\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        \n",
    "        # Sentiment line\n",
    "        plt.plot(monthly_sentiment['date'], monthly_sentiment[sentiment_col], \n",
    "                 marker='o', linestyle='-', color='blue', alpha=0.7)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Event annotations\n",
    "        for event in key_events:\n",
    "            event_date = pd.to_datetime(event['date'] + '-01')\n",
    "            \n",
    "            if event_date < monthly_sentiment['date'].min() or event_date > monthly_sentiment['date'].max():\n",
    "                continue\n",
    "                \n",
    "            # Closest sentiment value\n",
    "            closest_idx = (monthly_sentiment['date'] - event_date).abs().idxmin()\n",
    "            event_sentiment = monthly_sentiment.loc[closest_idx, sentiment_col]\n",
    "            \n",
    "            plt.annotate(event['event'], \n",
    "                        xy=(event_date, event_sentiment),\n",
    "                        xytext=(event_date, event_sentiment + event['y_pos']),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                        fontsize=10)\n",
    "        \n",
    "        plt.title('AI Sentiment Timeline with Key Events')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sentiment Score (-1 to 1)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_technology_timeline(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create timeline showing the adoption/mention of AI technologies\"\"\"\n",
    "        if 'ai_technologies' not in df.columns:\n",
    "            print(\"No technology data found for timeline\")\n",
    "            return plt.figure()\n",
    "        \n",
    "\n",
    "        tech_rows = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            date = row['date']\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            \n",
    "            for tech_category, tech_items in techs.items():\n",
    "                if isinstance(tech_items, list):\n",
    "                    for tech in tech_items:\n",
    "                        tech_rows.append({\n",
    "                            'date': date,\n",
    "                            'tech_category': tech_category,\n",
    "                            'tech_item': tech,\n",
    "                            'count': 1\n",
    "                        })\n",
    "                else:\n",
    "                    tech_rows.append({\n",
    "                        'date': date,\n",
    "                        'tech_category': tech_category,\n",
    "                        'tech_item': str(tech_items),\n",
    "                        'count': 1\n",
    "                    })\n",
    "        \n",
    "        # Dataframe\n",
    "        if not tech_rows:\n",
    "            return plt.figure()\n",
    "            \n",
    "        tech_df = pd.DataFrame(tech_rows)\n",
    "        \n",
    "        # Grouping by month and technology category\n",
    "        tech_df['yearmonth'] = tech_df['date'].dt.strftime('%Y-%m')\n",
    "        monthly_tech = tech_df.groupby(['yearmonth', 'tech_category']).size().reset_index(name='count')\n",
    "        \n",
    "        # Date\n",
    "        monthly_tech['date'] = pd.to_datetime(monthly_tech['yearmonth'] + '-01')\n",
    "        monthly_tech = monthly_tech.sort_values('date')\n",
    "        \n",
    "        # Adoption over time\n",
    "        monthly_tech['cumulative'] = monthly_tech.groupby('tech_category')['count'].cumsum()\n",
    "        \n",
    "        pivot_df = monthly_tech.pivot(index='date', columns='tech_category', values='cumulative').fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        pivot_df.plot(kind='line', figsize=(14, 7))\n",
    "        plt.title('AI Technology Adoption Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Cumulative Mentions')\n",
    "        plt.legend(title='Technology', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n",
    "    \n",
    "def _create_organization_ai_strategy_comparison(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Compare different organizations' AI strategies based on sentiment and technology focus\"\"\"\n",
    "        if 'top_organizations' not in df.columns or 'ai_technologies' not in df.columns:\n",
    "            print(\"Required columns missing for organization comparison\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        org_df = df.explode('top_organizations').dropna(subset=['top_organizations'])\n",
    "        \n",
    "        # Top organizations\n",
    "        top_orgs = org_df['top_organizations'].value_counts().head(10).index.tolist()\n",
    "        \n",
    "        top_org_df = org_df[org_df['top_organizations'].isin(top_orgs)]\n",
    "        \n",
    "        # Tech organization matrix\n",
    "        org_tech_data = []\n",
    "        \n",
    "        for _, row in top_org_df.iterrows():\n",
    "            org = row['top_organizations']\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            \n",
    "            if not techs:\n",
    "                continue\n",
    "                \n",
    "            # Get sentiment\n",
    "            sentiment = 0\n",
    "            if 'sentiment_overall' in row:\n",
    "                sentiment = row['sentiment_overall']\n",
    "            elif 'sentiment' in row:\n",
    "                sentiment = row['sentiment']\n",
    "            \n",
    "            # Tech categories\n",
    "            for tech in techs:\n",
    "                org_tech_data.append({\n",
    "                    'organization': org,\n",
    "                    'technology': tech,\n",
    "                    'sentiment': sentiment,\n",
    "                    'count': 1\n",
    "                })\n",
    "        \n",
    "        if not org_tech_data:\n",
    "            return plt.figure()\n",
    "            \n",
    "        org_tech_df = pd.DataFrame(org_tech_data)\n",
    "        \n",
    "        # Grouping by organization and tech\n",
    "        org_tech_summary = org_tech_df.groupby(['organization', 'technology']).agg(\n",
    "            sentiment=('sentiment', 'mean'),\n",
    "            count=('count', 'sum')\n",
    "        ).reset_index()\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Top technologies\n",
    "        top_techs = org_tech_df['technology'].value_counts().head(7).index.tolist()\n",
    "        \n",
    "        plot_data = org_tech_summary[org_tech_summary['technology'].isin(top_techs)]\n",
    "        \n",
    "        # Bubble chart\n",
    "        for tech in top_techs:\n",
    "            tech_data = plot_data[plot_data['technology'] == tech]\n",
    "            plt.scatter(tech_data['organization'], tech_data['sentiment'], \n",
    "                       s=tech_data['count'] * 50, alpha=0.7, label=tech)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.title('Organization AI Strategy Comparison')\n",
    "        plt.xlabel('Organization')\n",
    "        plt.ylabel('Sentiment Score (-1 to 1)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Technology Focus', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tech_job_impact_network(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create network visualization showing how technologies impact different job roles\"\"\"\n",
    "        if 'ai_technologies' not in df.columns or 'detected_jobs' not in df.columns:\n",
    "            print(\"Required columns missing for job impact network\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            jobs = row.get('detected_jobs', [])\n",
    "            \n",
    "            if not techs or not jobs:\n",
    "                continue\n",
    "            \n",
    "            # Sentiment\n",
    "            sentiment = 0 \n",
    "            if 'sentiment_overall' in row:\n",
    "                sentiment = row['sentiment_overall']\n",
    "            elif 'sentiment' in row:\n",
    "                sentiment = row['sentiment']\n",
    "            \n",
    "            # Edges between technologies and jobs\n",
    "            for tech_category in techs:\n",
    "                # Tech node\n",
    "                if not G.has_node(tech_category):\n",
    "                    G.add_node(tech_category, type='technology')\n",
    "                \n",
    "                # Job nodes\n",
    "                for job in jobs:\n",
    "                    if not G.has_node(job):\n",
    "                        G.add_node(job, type='job')\n",
    "                    \n",
    "                    if G.has_edge(tech_category, job):\n",
    "                        # Update with new sentiment info\n",
    "                        current = G[tech_category][job]\n",
    "                        current['weight'] = (current['weight'] * current['count'] + sentiment) / (current['count'] + 1)\n",
    "                        current['count'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(tech_category, job, weight=sentiment, count=1)\n",
    "        \n",
    "        if len(G.edges()) == 0:\n",
    "            return plt.figure()\n",
    "        \n",
    "        significant_edges = [(u, v) for u, v, d in G.edges(data=True) if d['count'] >= 2]\n",
    "        \n",
    "        # If there is no significant edges, I will use all edges\n",
    "        if not significant_edges:\n",
    "            significant_edges = G.edges()\n",
    "            \n",
    "        SG = G.edge_subgraph(significant_edges).copy()\n",
    "        \n",
    "        pos = nx.spring_layout(SG, seed=42)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Nodes for tech vs jobs\n",
    "        tech_nodes = [n for n, d in SG.nodes(data=True) if d.get('type') == 'technology']\n",
    "        job_nodes = [n for n, d in SG.nodes(data=True) if d.get('type') == 'job']\n",
    "        \n",
    "        # Size nodes by degree\n",
    "        tech_sizes = [SG.degree(n) * 50 for n in tech_nodes]\n",
    "        job_sizes = [SG.degree(n) * 50 for n in job_nodes]\n",
    "        \n",
    "        nx.draw_networkx_nodes(SG, pos, nodelist=tech_nodes, node_color='lightblue', \n",
    "                              node_size=tech_sizes, alpha=0.8)\n",
    "        nx.draw_networkx_nodes(SG, pos, nodelist=job_nodes, node_color='lightgreen',\n",
    "                              node_size=job_sizes, alpha=0.8)\n",
    "        \n",
    "        # Edges with colors based on sentiment\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        for u, v, d in SG.edges(data=True):\n",
    "            if d['weight'] > 0.2:\n",
    "                edge_colors.append('green')\n",
    "            elif d['weight'] < -0.2:\n",
    "                edge_colors.append('red')\n",
    "            else:\n",
    "                edge_colors.append('gray')\n",
    "            \n",
    "            # Edge width based on count\n",
    "            edge_widths.append(1 + d['count'] * 0.5)\n",
    "        \n",
    "        nx.draw_networkx_edges(SG, pos, width=edge_widths, edge_color=edge_colors, alpha=0.6)\n",
    "        \n",
    "        nx.draw_networkx_labels(SG, pos, font_size=10)\n",
    "        \n",
    "        plt.title('AI Technology Impact on Job Roles')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_topics_over_time(self, df: pd.DataFrame, topics: np.ndarray, top_n_topics: int = 5) -> plt.Figure:\n",
    "        \"\"\"Plot the prevalence of top topics over time\"\"\"\n",
    "        topic_df = pd.DataFrame({'date': df['date'], 'topic': topics})\n",
    "        \n",
    "        # Counts of each topic\n",
    "        topic_counts = Counter(topics)\n",
    "        top_topics = [topic for topic, count in topic_counts.most_common(top_n_topics) if topic != -1]\n",
    "        \n",
    "        # Top topics by month\n",
    "        topic_df = topic_df[topic_df['topic'].isin(top_topics)]\n",
    "        topic_df['yearmonth'] = topic_df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Topics per month\n",
    "        topic_time = topic_df.groupby(['yearmonth', 'topic']).size().reset_index(name='count')\n",
    "        \n",
    "        # Datetime\n",
    "        topic_time['date'] = pd.to_datetime(topic_time['yearmonth'] + '-01')\n",
    "        topic_time = topic_time.sort_values('date')\n",
    "        \n",
    "        pivot_df = topic_time.pivot(index='date', columns='topic', values='count').fillna(0)\n",
    "        \n",
    "        topic_labels = {}\n",
    "        if hasattr(self, 'topic_model'):\n",
    "            for topic in top_topics:\n",
    "                if topic != -1:  \n",
    "                    words = self.topic_model.get_topic(topic)\n",
    "                    if words:\n",
    "                        # Label from top 3 words\n",
    "                        label = ', '.join([word for word, _ in words[:3]])\n",
    "                        topic_labels[topic] = f\"Topic {topic}: {label}\"\n",
    "                    else:\n",
    "                        topic_labels[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        if topic_labels:\n",
    "            pivot_df = pivot_df.rename(columns=topic_labels)\n",
    "            \n",
    "        pivot_df.plot(kind='line', figsize=(12, 6))\n",
    "        plt.title('Top Topics Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Article Count')\n",
    "        plt.legend(title='Topic', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(self, \n",
    "                    sample_size: int = 10000,\n",
    "                    force_recompute: bool = False) -> Dict[str, plt.Figure]:\n",
    "        \n",
    "        # Cleaning the data\n",
    "        df_clean = self.clean_and_filter_data(force_recompute=force_recompute)\n",
    "        \n",
    "        if sample_size and sample_size < len(df_clean):\n",
    "            print(f\"Using sample of {sample_size} articles for analysis\")\n",
    "            df_sample = df_clean.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            df_sample = df_clean\n",
    "        \n",
    "        # Extracting the entities\n",
    "        entity_sample_size = min(5000, len(df_sample))\n",
    "        df_entities = self.extract_entities(df_sample, sample_size=entity_sample_size, \n",
    "                                           force_recompute=force_recompute)\n",
    "        \n",
    "        # Detecting the industries and jobs\n",
    "        df_classified = self.detect_industries_and_jobs(df_sample, force_recompute=force_recompute)\n",
    "        \n",
    "        # Identifying the technologies\n",
    "        df_tech = self.identify_technologies(df_classified, force_recompute=force_recompute)\n",
    "        \n",
    "        topic_model, topics = self.run_topic_modeling(df_tech, force_recompute=force_recompute)\n",
    "        self.topic_model = topic_model\n",
    "        self.topics = topics\n",
    "        \n",
    "        # Sentiment\n",
    "        sentiment_sample_size = min(10000, len(df_tech))\n",
    "        df_sentiment = self.analyze_sentiment(df_tech, sample_size=sentiment_sample_size, \n",
    "                                             force_recompute=force_recompute)\n",
    "        \n",
    "        figures = self.create_visualizations(df_sentiment)\n",
    "        \n",
    "        print(\"Analysis pipeline complete!\")\n",
    "        return figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpaCy model...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analyzer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m AINewsAnalyzer(data_path, cache_dir)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Running the pipeline with the sample size\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mAINewsAnalyzer.__init__\u001b[0;34m(self, data_path, cache_dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# SpaCy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading SpaCy model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Dictionaries for sentiment analysis\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_sentiment_dictionaries()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    data_path = 'https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet'\n",
    "    cache_dir = 'cache'\n",
    "    \n",
    "    analyzer = AINewsAnalyzer(data_path, cache_dir)\n",
    "    \n",
    "    # Running the pipeline with the sample size\n",
    "    sample_size = 20000\n",
    "    figures = analyzer.run_pipeline(sample_size=sample_size)\n",
    "    \n",
    "    output_dir = 'figures'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for name, fig in figures.items():\n",
    "        fig.savefig(f\"{output_dir}/{name}.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Saved {len(figures)} visualizations to {output_dir}/\")\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
