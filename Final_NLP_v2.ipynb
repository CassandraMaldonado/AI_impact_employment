{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding & Preparation\n",
    "\n",
    "1. I will load and explore the dataset structure.\n",
    "2. Then I will examine the articles content, sources, publication dates, and metadata.\n",
    "3. I will clean the data by removing web crawl artifacts (HTML tags, etc).\n",
    "4. I will filter out irrelevant articles that don't focus on AI's impact on industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200083, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200083 entries, 0 to 200082\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   url       200083 non-null  object\n",
      " 1   date      200083 non-null  object\n",
      " 2   language  200083 non-null  object\n",
      " 3   title     200083 non-null  object\n",
      " 4   text      200083 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://businessnewsthisweek.com/business/infog...</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Infogain AI Business Solutions Now Available i...</td>\n",
       "      <td>\\n\\nInfogain AI Business Solutions Now Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://allafrica.com/stories/202504250184.html</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Africa: AI Policies in Africa - Lessons From G...</td>\n",
       "      <td>\\nAfrica: AI Policies in Africa - Lessons From...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://asiatimes.com/2023/07/yang-lan-intervi...</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Yang Lan interviews academics on AI developmen...</td>\n",
       "      <td>\\nYang Lan interviews academics on AI developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.meritalk.com/articles/commerce-nom...</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>en</td>\n",
       "      <td>Commerce Nominee Promises Increased Domestic A...</td>\n",
       "      <td>\\nCommerce Nominee Promises Increased Domestic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://citylife.capetown/hmn/uncategorized/re...</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry: Th...</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language  \\\n",
       "0  http://businessnewsthisweek.com/business/infog...  2023-05-20       en   \n",
       "1    https://allafrica.com/stories/202504250184.html  2025-04-25       en   \n",
       "2  https://asiatimes.com/2023/07/yang-lan-intervi...  2023-07-25       en   \n",
       "3  https://cdn.meritalk.com/articles/commerce-nom...  2025-02-04       en   \n",
       "4  https://citylife.capetown/hmn/uncategorized/re...  2023-11-11       en   \n",
       "\n",
       "                                               title  \\\n",
       "0  Infogain AI Business Solutions Now Available i...   \n",
       "1  Africa: AI Policies in Africa - Lessons From G...   \n",
       "2  Yang Lan interviews academics on AI developmen...   \n",
       "3  Commerce Nominee Promises Increased Domestic A...   \n",
       "4  Revolutionizing the Manufacturing Industry: Th...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nInfogain AI Business Solutions Now Availab...  \n",
       "1  \\nAfrica: AI Policies in Africa - Lessons From...  \n",
       "2  \\nYang Lan interviews academics on AI developm...  \n",
       "3  \\nCommerce Nominee Promises Increased Domestic...  \n",
       "4     Revolutionizing the Manufacturing Industry:...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning removing html, punctuation, and stop words\n",
    "def clean_article(text):\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # Removing HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Removing the stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "    return text\n",
    "\n",
    "df['cleaned_article'] = df['text'].apply(clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.dropna(subset=['date'])  \n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['yearmonth'] = df['date'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance Filtering checking if the article discusses AI impact on industries or jobs\n",
    "def is_relevant(text):\n",
    "    keywords = ['AI', 'artificial intelligence', 'impact', 'industries', 'jobs', 'employment']\n",
    "    text = text.lower()\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "df_relevant = df[df['cleaned_article'].apply(is_relevant)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(df_relevant['cleaned_article'].tolist())\n",
    "#topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Industries\n",
    "def extract_industries(text):\n",
    "    industries = ['healthcare', 'finance', 'manufacturing', 'transportation', 'education']\n",
    "    text = text.lower()\n",
    "    found_industries = [industry for industry in industries if industry in text]\n",
    "    return found_industries\n",
    "\n",
    "df_relevant['industries'] = df_relevant['cleaned_article'].apply(extract_industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis for AI adoption in specific industries\n",
    "def analyze_sentiment(text, industry):\n",
    "    from textblob import TextBlob\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if industry:\n",
    "        return sentiment * len(industry) \n",
    "    return sentiment\n",
    "df_relevant['sentiment'] = df_relevant.apply(lambda x: analyze_sentiment(x['cleaned_article'], x['industries']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline Analysis\n",
    "df_relevant['date'] = pd.to_datetime(df_relevant['date'])\n",
    "\n",
    "# Exploding the industries column to handle lists\n",
    "df_exploded = df_relevant.explode('industries')\n",
    "\n",
    "# Grouping by date and industries to calculate mean sentiment\n",
    "sentiment_over_time = df_exploded.groupby(['date', 'industries'])['sentiment'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying AI technologies mentioned\n",
    "def extract_technologies(text):\n",
    "    technologies = ['machine learning', 'deep learning', 'natural language processing', 'computer vision']\n",
    "    text = text.lower()\n",
    "    found_technologies = [tech for tech in technologies if tech in text]\n",
    "    return found_technologies\n",
    "\n",
    "df_relevant['technologies'] = df_relevant['cleaned_article'].apply(extract_technologies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
