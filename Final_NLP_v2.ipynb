{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML & NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding & Preparation\n",
    "\n",
    "1. I will load and explore the dataset structure.\n",
    "2. Then I will examine the articles content, sources, publication dates, and metadata.\n",
    "3. I will clean the data by removing web crawl artifacts (HTML tags, etc).\n",
    "4. I will filter out irrelevant articles that don't focus on AI's impact on industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200083, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200083 entries, 0 to 200082\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   url       200083 non-null  object\n",
      " 1   date      200083 non-null  object\n",
      " 2   language  200083 non-null  object\n",
      " 3   title     200083 non-null  object\n",
      " 4   text      200083 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://businessnewsthisweek.com/business/infog...</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Infogain AI Business Solutions Now Available i...</td>\n",
       "      <td>\\n\\nInfogain AI Business Solutions Now Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://allafrica.com/stories/202504250184.html</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Africa: AI Policies in Africa - Lessons From G...</td>\n",
       "      <td>\\nAfrica: AI Policies in Africa - Lessons From...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://asiatimes.com/2023/07/yang-lan-intervi...</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Yang Lan interviews academics on AI developmen...</td>\n",
       "      <td>\\nYang Lan interviews academics on AI developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.meritalk.com/articles/commerce-nom...</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>en</td>\n",
       "      <td>Commerce Nominee Promises Increased Domestic A...</td>\n",
       "      <td>\\nCommerce Nominee Promises Increased Domestic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://citylife.capetown/hmn/uncategorized/re...</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry: Th...</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language  \\\n",
       "0  http://businessnewsthisweek.com/business/infog...  2023-05-20       en   \n",
       "1    https://allafrica.com/stories/202504250184.html  2025-04-25       en   \n",
       "2  https://asiatimes.com/2023/07/yang-lan-intervi...  2023-07-25       en   \n",
       "3  https://cdn.meritalk.com/articles/commerce-nom...  2025-02-04       en   \n",
       "4  https://citylife.capetown/hmn/uncategorized/re...  2023-11-11       en   \n",
       "\n",
       "                                               title  \\\n",
       "0  Infogain AI Business Solutions Now Available i...   \n",
       "1  Africa: AI Policies in Africa - Lessons From G...   \n",
       "2  Yang Lan interviews academics on AI developmen...   \n",
       "3  Commerce Nominee Promises Increased Domestic A...   \n",
       "4  Revolutionizing the Manufacturing Industry: Th...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nInfogain AI Business Solutions Now Availab...  \n",
       "1  \\nAfrica: AI Policies in Africa - Lessons From...  \n",
       "2  \\nYang Lan interviews academics on AI developm...  \n",
       "3  \\nCommerce Nominee Promises Increased Domestic...  \n",
       "4     Revolutionizing the Manufacturing Industry:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AINewsAnalyzer:\n",
    "    \"\"\"Main class for analyzing AI-related news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, cache_dir: str = \"cache\"):\n",
    "        \"\"\"Initialize the analyzer with data path and cache directory\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        \n",
    "        # SpaCy\n",
    "        print(\"Loading SpaCy model...\")\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        # Dictionaries for sentiment analysis\n",
    "        self._create_sentiment_dictionaries()\n",
    "        \n",
    "        # Data\n",
    "        self.df = self._load_data()\n",
    "        print(f\"Loaded dataset with {len(self.df)} articles\")\n",
    "    \n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the dataset from parquet file\"\"\"\n",
    "        return pd.read_parquet(self.data_path, engine='pyarrow')\n",
    "    \n",
    "    def _get_cache_path(self, filename: str) -> str:\n",
    "        \"\"\"Get full path for a cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, filename)\n",
    "    \n",
    "    def _save_to_cache(self, obj: Any, filename: str) -> None:\n",
    "        \"\"\"Save object to cache\"\"\"\n",
    "        with open(self._get_cache_path(filename), 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "    \n",
    "    def _load_from_cache(self, filename: str) -> Any:\n",
    "        \"\"\"Load object from cache if it exists\"\"\"\n",
    "        cache_path = self._get_cache_path(filename)\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    def _create_sentiment_dictionaries(self) -> None:\n",
    "        \"\"\"Create dictionaries for domain-specific sentiment analysis\"\"\"\n",
    "        # Positive terms related to AI\n",
    "        self.positive_terms = {\n",
    "            'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7, \n",
    "            'assist': 0.6, 'empower': 0.9, 'efficiency': 0.8, 'productivity': 0.8, \n",
    "            'innovation': 0.9, 'growth': 0.7, 'advancement': 0.8, 'collaborate': 0.7, \n",
    "            'partnership': 0.6, 'upskill': 0.9, 'complement': 0.7, 'benefit': 0.8,\n",
    "            'progress': 0.7, 'create': 0.6, 'advantage': 0.7, 'potential': 0.5,\n",
    "            'solution': 0.6, 'revolutionize': 0.8\n",
    "        }\n",
    "        \n",
    "        # Negative terms related to AI\n",
    "        self.negative_terms = {\n",
    "            'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'threaten': -0.7, \n",
    "            'risk': -0.6, 'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, \n",
    "            'downsizing': -0.8, 'automation': -0.5, 'obsolete': -0.8, 'disruption': -0.6, \n",
    "            'inequality': -0.7, 'bias': -0.7, 'surveillance': -0.8, 'danger': -0.7,\n",
    "            'concern': -0.5, 'worry': -0.6, 'fear': -0.7, 'threat': -0.8,\n",
    "            'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(self, \n",
    "                              force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main function to clean and filter the dataset\n",
    "        \n",
    "        Args:\n",
    "            force_recompute: Whether to force recomputation even if cached results exist\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cleaned and filtered data\n",
    "        \"\"\"\n",
    "        cache_file = \"cleaned_data.pkl\"\n",
    "        \n",
    "        if not force_recompute:\n",
    "            df_clean = self._load_from_cache(cache_file)\n",
    "            if df_clean is not None:\n",
    "                print(\"Loaded cleaned data from cache\")\n",
    "                return df_clean\n",
    "        \n",
    "        print(\"Cleaning and filtering data...\")\n",
    "        \n",
    "        # Clean text\n",
    "        self.df['cleaned_text'] = self.df['text'].apply(self._clean_article)\n",
    "        \n",
    "        # Parsing dates\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=['date'])\n",
    "        \n",
    "        # Time features\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        self.df['yearmonth'] = self.df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Relevance\n",
    "        self.df['is_relevant'] = self.df['cleaned_text'].apply(self._is_relevant)\n",
    "        df_relevant = self.df[self.df['is_relevant']].copy()\n",
    "        \n",
    "        # Extract for source analysis\n",
    "        df_relevant['source_domain'] = df_relevant['url'].apply(self._extract_domain)\n",
    "        \n",
    "        self._save_to_cache(df_relevant, cache_file)\n",
    "        \n",
    "        print(f\"Filtered to {len(df_relevant)} relevant articles\")\n",
    "        return df_relevant\n",
    "    \n",
    "def _clean_article(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean article text by removing HTML, extra whitespace, etc.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw article text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Handling none or empty strings\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Removing the HTML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        \n",
    "        # Removing the URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Removing the extra whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Removing the special characters\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:\\'\\\"()-]', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "def _extract_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        if not url or pd.isna(url):\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Domain using regex\n",
    "            domain_match = re.search(r'https?://(?:www\\.)?([^/]+)', url)\n",
    "            if domain_match:\n",
    "                return domain_match.group(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "def _is_relevant(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if article is relevant to AI's impact on industries/jobs\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned article text\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating relevance\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Checking for AI related terms\n",
    "        ai_terms = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', \n",
    "                   'neural network', 'llm', 'large language model', 'chatgpt', 'generative ai']\n",
    "        \n",
    "        # Checking for industry impact terms\n",
    "        impact_terms = ['impact', 'effect', 'transform', 'disrupt', 'replace', 'automate',\n",
    "                       'job', 'employment', 'workforce', 'career', 'industry', 'sector', \n",
    "                       'profession', 'work', 'labor market', 'skill']\n",
    "        \n",
    "        contains_ai = any(term in text_lower for term in ai_terms)\n",
    "        contains_impact = any(term in text_lower for term in impact_terms)\n",
    "        \n",
    "        if not (contains_ai and contains_impact):\n",
    "            return False\n",
    "        \n",
    "        #Proximity within same paragraph for better accuracy\n",
    "        paragraphs = text_lower.split('\\n')\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_has_ai = any(term in para for term in ai_terms)\n",
    "            para_has_impact = any(term in para for term in impact_terms)\n",
    "            \n",
    "            if para_has_ai and para_has_impact:\n",
    "                return True\n",
    "        \n",
    "        # Fallback for short texts without paragraphs, checking the  sentence proximity\n",
    "        sentences = text_lower.split('.')\n",
    "        \n",
    "        ai_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in ai_terms)]\n",
    "        impact_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in impact_terms)]\n",
    "        \n",
    "        # AI and impact sentences are close to each other within 3 sentences\n",
    "        for ai_idx in ai_sentences:\n",
    "            for impact_idx in impact_sentences:\n",
    "                if abs(ai_idx - impact_idx) <= 3:\n",
    "                    return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing the articles with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory for the \n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing with SpaCy\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Adding the entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separate columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Count frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry and Job Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for the entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory\n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separatating columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_technologies(self, \n",
    "                             df: pd.DataFrame,\n",
    "                             force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identify AI technologies mentioned in articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with identified technologies\n",
    "        \"\"\"\n",
    "        cache_file = \"tech_data.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_tech = self._load_from_cache(cache_file)\n",
    "            if df_tech is not None:\n",
    "                print(\"Loaded technology data from cache\")\n",
    "                return df_tech\n",
    "        \n",
    "        print(\"Identifying AI technologies...\")\n",
    "        \n",
    "        # Technology categories and it's keywords\n",
    "        technologies = {\n",
    "            'machine_learning': [\n",
    "                'machine learning', 'ml', 'supervised learning', 'unsupervised learning', \n",
    "                'reinforcement learning', 'decision trees', 'random forests', 'svm'\n",
    "            ],\n",
    "            'deep_learning': [\n",
    "                'deep learning', 'neural networks', 'cnn', 'rnn', 'lstm', 'transformer models',\n",
    "                'generative ai', 'gan', 'diffusion models'\n",
    "            ],\n",
    "            'nlp': [\n",
    "                'natural language processing', 'nlp', 'language models', 'llm', 'chatbots',\n",
    "                'sentiment analysis', 'named entity recognition', 'text classification'\n",
    "            ],\n",
    "            'computer_vision': [\n",
    "                'computer vision', 'image recognition', 'object detection', 'facial recognition',\n",
    "                'image segmentation', 'video analysis'\n",
    "            ],\n",
    "            'robotics': [\n",
    "                'robotics', 'robots', 'automation', 'robotic process automation', 'rpa',\n",
    "                'autonomous systems', 'drones', 'self-driving'\n",
    "            ],\n",
    "            'voice_ai': [\n",
    "                'voice assistant', 'speech recognition', 'text to speech', 'voice synthesis',\n",
    "                'voice computing', 'speech to text'\n",
    "            ],\n",
    "            'ai_infrastructure': [\n",
    "                'gpu', 'tpu', 'cloud computing', 'ai chips', 'neural processors',\n",
    "                'edge ai', 'ai hardware', 'quantum computing'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Model names\n",
    "        ai_models = [\n",
    "            'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'llama', 'gemini', 'claude', 'bert', \n",
    "            'stable diffusion', 'dall-e', 'midjourney', 'bard', 'palm', 'chinchilla'\n",
    "        ]\n",
    "        \n",
    "        # Technology identification\n",
    "        df['ai_technologies'] = df['cleaned_text'].apply(\n",
    "            lambda x: self._identify_technologies(x, technologies, ai_models)\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(df, cache_file)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def _identify_technologies(self, \n",
    "                              text: str, \n",
    "                              technologies: Dict[str, List[str]], \n",
    "                              ai_models: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Identify AI technologies mentioned in the text\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        found_techs = {}\n",
    "        # Technology categories\n",
    "        for tech_category, keywords in technologies.items():\n",
    "            matched_keywords = [k for k in keywords if k in text_lower]\n",
    "            if matched_keywords:\n",
    "                found_techs[tech_category] = matched_keywords\n",
    "        \n",
    "        # AI models\n",
    "        found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "        if found_models:\n",
    "            found_techs['specific_models'] = found_models\n",
    "        \n",
    "        return found_techs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 10000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyze sentiment of articles regarding AI impact\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with sentiment analysis\n",
    "        \"\"\"\n",
    "        cache_file = f\"sentiment_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_sentiment = self._load_from_cache(cache_file)\n",
    "            if df_sentiment is not None:\n",
    "                print(f\"Loaded sentiment data for {len(df_sentiment)} articles from cache\")\n",
    "                return df_sentiment\n",
    "        \n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for sentiment analysis\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Analyzing sentiment...\")\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sample_df['sentiment_scores'] = sample_df.apply(\n",
    "            lambda x: self._analyze_contextual_sentiment(\n",
    "                x['cleaned_text'], \n",
    "                x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "            ), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Extracting the components of sentiment\n",
    "        sample_df['sentiment_overall'] = sample_df['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "        sample_df['sentiment_ai_impact'] = sample_df['sentiment_scores'].apply(lambda x: x['ai_impact'])\n",
    "        sample_df['sentiment_industry'] = sample_df['sentiment_scores'].apply(lambda x: x['industry_context'])\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _analyze_contextual_sentiment(self, \n",
    "                                     text: str, \n",
    "                                     industry: Optional[str] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze sentiment with context awareness for AI impact\n",
    "        \n",
    "        Args:\n",
    "            text: Article text\n",
    "            industry: Specific industry to contextualize sentiment\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with different sentiment scores\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {'overall': 0, 'ai_impact': 0, 'industry_context': 0}\n",
    "        \n",
    "        # Base sentiment with TextBlob\n",
    "        base_sentiment = TextBlob(text).sentiment.polarity\n",
    "        \n",
    "        # AI impact sentiment using my custom dictionaries\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Occurrences and getting the weighted sentiment for domain-specific terms\n",
    "        positive_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.positive_terms.items()\n",
    "        )\n",
    "        \n",
    "        negative_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.negative_terms.items()\n",
    "        )\n",
    "        \n",
    "        # Domain-specific sentiment\n",
    "        ai_impact_mentions = sum(text_lower.count(term) for term in self.positive_terms) + \\\n",
    "                             sum(text_lower.count(term) for term in self.negative_terms)\n",
    "        \n",
    "        if ai_impact_mentions > 0:\n",
    "            ai_impact_sentiment = (positive_sentiment + negative_sentiment) / ai_impact_mentions\n",
    "        else:\n",
    "            ai_impact_sentiment = 0\n",
    "        \n",
    "        # Industry context sentiment\n",
    "        industry_context_sentiment = 0\n",
    "        if industry:\n",
    "            # Industry-specific terms\n",
    "            industry_terms = []\n",
    "            \n",
    "            if industry == 'healthcare':\n",
    "                industry_terms = ['patient', 'doctor', 'nurse', 'hospital', 'medical', 'diagnosis', 'treatment']\n",
    "            elif industry == 'finance':\n",
    "                industry_terms = ['bank', 'investment', 'trading', 'financial', 'insurance', 'loan']\n",
    "            elif industry == 'manufacturing':\n",
    "                industry_terms = ['factory', 'production', 'assembly', 'industrial', 'manufacturing']\n",
    "            elif industry == 'technology':\n",
    "                industry_terms = ['software', 'hardware', 'startup', 'tech', 'computing', 'digital']\n",
    "            elif industry == 'education':\n",
    "                industry_terms = ['student', 'teacher', 'school', 'learning', 'education', 'university']\n",
    "            \n",
    "            if industry_terms:\n",
    "                # Finding sentences containing industry terms\n",
    "                sentences = text.split('.')\n",
    "                industry_sentences = [s for s in sentences if any(term in s.lower() for term in industry_terms)]\n",
    "                \n",
    "                if industry_sentences:\n",
    "                    # Sentiment for industry-specific sentences\n",
    "                    industry_sentiment = np.mean([TextBlob(s).sentiment.polarity for s in industry_sentences])\n",
    "                    industry_context_sentiment = industry_sentiment\n",
    "        \n",
    "        # Sentiment is a weighted average of the base sentiment and AI impact sentiment\n",
    "        overall_sentiment = 0.4 * base_sentiment + 0.6 * ai_impact_sentiment\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_sentiment,\n",
    "            'ai_impact': ai_impact_sentiment,\n",
    "            'industry_context': industry_context_sentiment\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
