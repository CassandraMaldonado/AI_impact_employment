{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML & NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding & Preparation\n",
    "\n",
    "1. I will load and explore the dataset structure.\n",
    "2. Then I will examine the articles content, sources, publication dates, and metadata.\n",
    "3. I will clean the data by removing web crawl artifacts (HTML tags, etc).\n",
    "4. I will filter out irrelevant articles that don't focus on AI's impact on industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200083, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200083 entries, 0 to 200082\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   url       200083 non-null  object\n",
      " 1   date      200083 non-null  object\n",
      " 2   language  200083 non-null  object\n",
      " 3   title     200083 non-null  object\n",
      " 4   text      200083 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://businessnewsthisweek.com/business/infog...</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Infogain AI Business Solutions Now Available i...</td>\n",
       "      <td>\\n\\nInfogain AI Business Solutions Now Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://allafrica.com/stories/202504250184.html</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Africa: AI Policies in Africa - Lessons From G...</td>\n",
       "      <td>\\nAfrica: AI Policies in Africa - Lessons From...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://asiatimes.com/2023/07/yang-lan-intervi...</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Yang Lan interviews academics on AI developmen...</td>\n",
       "      <td>\\nYang Lan interviews academics on AI developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.meritalk.com/articles/commerce-nom...</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>en</td>\n",
       "      <td>Commerce Nominee Promises Increased Domestic A...</td>\n",
       "      <td>\\nCommerce Nominee Promises Increased Domestic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://citylife.capetown/hmn/uncategorized/re...</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry: Th...</td>\n",
       "      <td>Revolutionizing the Manufacturing Industry:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language  \\\n",
       "0  http://businessnewsthisweek.com/business/infog...  2023-05-20       en   \n",
       "1    https://allafrica.com/stories/202504250184.html  2025-04-25       en   \n",
       "2  https://asiatimes.com/2023/07/yang-lan-intervi...  2023-07-25       en   \n",
       "3  https://cdn.meritalk.com/articles/commerce-nom...  2025-02-04       en   \n",
       "4  https://citylife.capetown/hmn/uncategorized/re...  2023-11-11       en   \n",
       "\n",
       "                                               title  \\\n",
       "0  Infogain AI Business Solutions Now Available i...   \n",
       "1  Africa: AI Policies in Africa - Lessons From G...   \n",
       "2  Yang Lan interviews academics on AI developmen...   \n",
       "3  Commerce Nominee Promises Increased Domestic A...   \n",
       "4  Revolutionizing the Manufacturing Industry: Th...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nInfogain AI Business Solutions Now Availab...  \n",
       "1  \\nAfrica: AI Policies in Africa - Lessons From...  \n",
       "2  \\nYang Lan interviews academics on AI developm...  \n",
       "3  \\nCommerce Nominee Promises Increased Domestic...  \n",
       "4     Revolutionizing the Manufacturing Industry:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AINewsAnalyzer:\n",
    "    \"\"\"Main class for analyzing AI-related news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, cache_dir: str = \"cache\"):\n",
    "        \"\"\"Initialize the analyzer with data path and cache directory\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        \n",
    "        # SpaCy\n",
    "        print(\"Loading SpaCy model...\")\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        # Dictionaries for sentiment analysis\n",
    "        self._create_sentiment_dictionaries()\n",
    "        \n",
    "        # Data\n",
    "        self.df = self._load_data()\n",
    "        print(f\"Loaded dataset with {len(self.df)} articles\")\n",
    "    \n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the dataset from parquet file\"\"\"\n",
    "        return pd.read_parquet(self.data_path, engine='pyarrow')\n",
    "    \n",
    "    def _get_cache_path(self, filename: str) -> str:\n",
    "        \"\"\"Get full path for a cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, filename)\n",
    "    \n",
    "    def _save_to_cache(self, obj: Any, filename: str) -> None:\n",
    "        \"\"\"Save object to cache\"\"\"\n",
    "        with open(self._get_cache_path(filename), 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "    \n",
    "    def _load_from_cache(self, filename: str) -> Any:\n",
    "        \"\"\"Load object from cache if it exists\"\"\"\n",
    "        cache_path = self._get_cache_path(filename)\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    def _create_sentiment_dictionaries(self) -> None:\n",
    "        \"\"\"Create dictionaries for domain-specific sentiment analysis\"\"\"\n",
    "        # Positive terms related to AI\n",
    "        self.positive_terms = {\n",
    "            'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7, \n",
    "            'assist': 0.6, 'empower': 0.9, 'efficiency': 0.8, 'productivity': 0.8, \n",
    "            'innovation': 0.9, 'growth': 0.7, 'advancement': 0.8, 'collaborate': 0.7, \n",
    "            'partnership': 0.6, 'upskill': 0.9, 'complement': 0.7, 'benefit': 0.8,\n",
    "            'progress': 0.7, 'create': 0.6, 'advantage': 0.7, 'potential': 0.5,\n",
    "            'solution': 0.6, 'revolutionize': 0.8\n",
    "        }\n",
    "        \n",
    "        # Negative terms related to AI\n",
    "        self.negative_terms = {\n",
    "            'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'threaten': -0.7, \n",
    "            'risk': -0.6, 'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, \n",
    "            'downsizing': -0.8, 'automation': -0.5, 'obsolete': -0.8, 'disruption': -0.6, \n",
    "            'inequality': -0.7, 'bias': -0.7, 'surveillance': -0.8, 'danger': -0.7,\n",
    "            'concern': -0.5, 'worry': -0.6, 'fear': -0.7, 'threat': -0.8,\n",
    "            'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(self, \n",
    "                              force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main function to clean and filter the dataset\n",
    "        \n",
    "        Args:\n",
    "            force_recompute: Whether to force recomputation even if cached results exist\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cleaned and filtered data\n",
    "        \"\"\"\n",
    "        cache_file = \"cleaned_data.pkl\"\n",
    "        \n",
    "        if not force_recompute:\n",
    "            df_clean = self._load_from_cache(cache_file)\n",
    "            if df_clean is not None:\n",
    "                print(\"Loaded cleaned data from cache\")\n",
    "                return df_clean\n",
    "        \n",
    "        print(\"Cleaning and filtering data...\")\n",
    "        \n",
    "        # Clean text\n",
    "        self.df['cleaned_text'] = self.df['text'].apply(self._clean_article)\n",
    "        \n",
    "        # Parsing dates\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=['date'])\n",
    "        \n",
    "        # Time features\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        self.df['yearmonth'] = self.df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Relevance\n",
    "        self.df['is_relevant'] = self.df['cleaned_text'].apply(self._is_relevant)\n",
    "        df_relevant = self.df[self.df['is_relevant']].copy()\n",
    "        \n",
    "        # Extract for source analysis\n",
    "        df_relevant['source_domain'] = df_relevant['url'].apply(self._extract_domain)\n",
    "        \n",
    "        self._save_to_cache(df_relevant, cache_file)\n",
    "        \n",
    "        print(f\"Filtered to {len(df_relevant)} relevant articles\")\n",
    "        return df_relevant\n",
    "    \n",
    "def _clean_article(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean article text by removing HTML, extra whitespace, etc.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw article text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Handling none or empty strings\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Removing the HTML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        \n",
    "        # Removing the URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Removing the extra whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Removing the special characters\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:\\'\\\"()-]', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "def _extract_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        if not url or pd.isna(url):\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Domain using regex\n",
    "            domain_match = re.search(r'https?://(?:www\\.)?([^/]+)', url)\n",
    "            if domain_match:\n",
    "                return domain_match.group(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "def _is_relevant(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if article is relevant to AI's impact on industries/jobs\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned article text\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating relevance\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Checking for AI related terms\n",
    "        ai_terms = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', \n",
    "                   'neural network', 'llm', 'large language model', 'chatgpt', 'generative ai']\n",
    "        \n",
    "        # Checking for industry impact terms\n",
    "        impact_terms = ['impact', 'effect', 'transform', 'disrupt', 'replace', 'automate',\n",
    "                       'job', 'employment', 'workforce', 'career', 'industry', 'sector', \n",
    "                       'profession', 'work', 'labor market', 'skill']\n",
    "        \n",
    "        contains_ai = any(term in text_lower for term in ai_terms)\n",
    "        contains_impact = any(term in text_lower for term in impact_terms)\n",
    "        \n",
    "        if not (contains_ai and contains_impact):\n",
    "            return False\n",
    "        \n",
    "        #Proximity within same paragraph for better accuracy\n",
    "        paragraphs = text_lower.split('\\n')\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_has_ai = any(term in para for term in ai_terms)\n",
    "            para_has_impact = any(term in para for term in impact_terms)\n",
    "            \n",
    "            if para_has_ai and para_has_impact:\n",
    "                return True\n",
    "        \n",
    "        # Fallback for short texts without paragraphs, checking the  sentence proximity\n",
    "        sentences = text_lower.split('.')\n",
    "        \n",
    "        ai_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in ai_terms)]\n",
    "        impact_sentences = [i for i, sent in enumerate(sentences) if any(term in sent for term in impact_terms)]\n",
    "        \n",
    "        # AI and impact sentences are close to each other within 3 sentences\n",
    "        for ai_idx in ai_sentences:\n",
    "            for impact_idx in impact_sentences:\n",
    "                if abs(ai_idx - impact_idx) <= 3:\n",
    "                    return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing the articles with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory for the \n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing with SpaCy\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Adding the entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separate columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Count frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry and Job Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 5000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract named entities from articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process (SpaCy is computationally expensive)\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted entities\n",
    "        \"\"\"\n",
    "        cache_file = f\"entity_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_entities = self._load_from_cache(cache_file)\n",
    "            if df_entities is not None:\n",
    "                print(f\"Loaded entity data for {len(df_entities)} articles from cache\")\n",
    "                return df_entities\n",
    "        \n",
    "        # Sample for the entity extraction\n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for entity extraction\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Extracting named entities...\")\n",
    "        \n",
    "        # Processing with SpaCy\n",
    "        entities_list = []\n",
    "        for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "            # Limiting the text length to manage memory\n",
    "            text = row['cleaned_text'][:10000]\n",
    "            \n",
    "            # Processing\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extracting the entities\n",
    "            entities = self._extract_entities_from_doc(doc)\n",
    "            entities_list.append(entities)\n",
    "        \n",
    "        # Entities\n",
    "        sample_df['extracted_entities'] = entities_list\n",
    "        \n",
    "        # Separatating columns for the top entities\n",
    "        sample_df['top_organizations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'organizations')\n",
    "        )\n",
    "        sample_df['top_people'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'people')\n",
    "        )\n",
    "        sample_df['top_locations'] = sample_df['extracted_entities'].apply(\n",
    "            lambda x: self._get_top_entities(x, 'locations')\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _extract_entities_from_doc(self, doc: Doc) -> Dict[str, Counter]:\n",
    "        \"\"\"Extract organizations, people, locations from SpaCy Doc\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ == 'PRODUCT':\n",
    "                entities['products'].append(ent.text)\n",
    "        \n",
    "        # Frequencies\n",
    "        entity_counts = {}\n",
    "        for entity_type, items in entities.items():\n",
    "            entity_counts[entity_type] = Counter(items)\n",
    "        \n",
    "        return entity_counts\n",
    "    \n",
    "def _get_top_entities(self, entity_dict: Dict[str, Counter], entity_type: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Get top n entities of a specific type\"\"\"\n",
    "        if entity_type not in entity_dict:\n",
    "            return []\n",
    "        \n",
    "        return [item for item, count in entity_dict[entity_type].most_common(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_technologies(self, \n",
    "                             df: pd.DataFrame,\n",
    "                             force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identify AI technologies mentioned in articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with identified technologies\n",
    "        \"\"\"\n",
    "        cache_file = \"tech_data.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_tech = self._load_from_cache(cache_file)\n",
    "            if df_tech is not None:\n",
    "                print(\"Loaded technology data from cache\")\n",
    "                return df_tech\n",
    "        \n",
    "        print(\"Identifying AI technologies...\")\n",
    "        \n",
    "        # Technology categories and it's keywords\n",
    "        technologies = {\n",
    "            'machine_learning': [\n",
    "                'machine learning', 'ml', 'supervised learning', 'unsupervised learning', \n",
    "                'reinforcement learning', 'decision trees', 'random forests', 'svm'\n",
    "            ],\n",
    "            'deep_learning': [\n",
    "                'deep learning', 'neural networks', 'cnn', 'rnn', 'lstm', 'transformer models',\n",
    "                'generative ai', 'gan', 'diffusion models'\n",
    "            ],\n",
    "            'nlp': [\n",
    "                'natural language processing', 'nlp', 'language models', 'llm', 'chatbots',\n",
    "                'sentiment analysis', 'named entity recognition', 'text classification'\n",
    "            ],\n",
    "            'computer_vision': [\n",
    "                'computer vision', 'image recognition', 'object detection', 'facial recognition',\n",
    "                'image segmentation', 'video analysis'\n",
    "            ],\n",
    "            'robotics': [\n",
    "                'robotics', 'robots', 'automation', 'robotic process automation', 'rpa',\n",
    "                'autonomous systems', 'drones', 'self-driving'\n",
    "            ],\n",
    "            'voice_ai': [\n",
    "                'voice assistant', 'speech recognition', 'text to speech', 'voice synthesis',\n",
    "                'voice computing', 'speech to text'\n",
    "            ],\n",
    "            'ai_infrastructure': [\n",
    "                'gpu', 'tpu', 'cloud computing', 'ai chips', 'neural processors',\n",
    "                'edge ai', 'ai hardware', 'quantum computing'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Model names\n",
    "        ai_models = [\n",
    "            'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'llama', 'gemini', 'claude', 'bert', \n",
    "            'stable diffusion', 'dall-e', 'midjourney', 'bard', 'palm', 'chinchilla'\n",
    "        ]\n",
    "        \n",
    "        # Technology identification\n",
    "        df['ai_technologies'] = df['cleaned_text'].apply(\n",
    "            lambda x: self._identify_technologies(x, technologies, ai_models)\n",
    "        )\n",
    "        \n",
    "        self._save_to_cache(df, cache_file)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def _identify_technologies(self, \n",
    "                              text: str, \n",
    "                              technologies: Dict[str, List[str]], \n",
    "                              ai_models: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Identify AI technologies mentioned in the text\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        found_techs = {}\n",
    "        # Technology categories\n",
    "        for tech_category, keywords in technologies.items():\n",
    "            matched_keywords = [k for k in keywords if k in text_lower]\n",
    "            if matched_keywords:\n",
    "                found_techs[tech_category] = matched_keywords\n",
    "        \n",
    "        # AI models\n",
    "        found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "        if found_models:\n",
    "            found_techs['specific_models'] = found_models\n",
    "        \n",
    "        return found_techs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         sample_size: int = 10000,\n",
    "                         force_recompute: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyze sentiment of articles regarding AI impact\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            sample_size: Number of articles to process\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with sentiment analysis\n",
    "        \"\"\"\n",
    "        cache_file = f\"sentiment_sample_{sample_size}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            df_sentiment = self._load_from_cache(cache_file)\n",
    "            if df_sentiment is not None:\n",
    "                print(f\"Loaded sentiment data for {len(df_sentiment)} articles from cache\")\n",
    "                return df_sentiment\n",
    "        \n",
    "        if sample_size and sample_size < len(df):\n",
    "            print(f\"Taking sample of {sample_size} articles for sentiment analysis\")\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "        \n",
    "        print(\"Analyzing sentiment...\")\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sample_df['sentiment_scores'] = sample_df.apply(\n",
    "            lambda x: self._analyze_contextual_sentiment(\n",
    "                x['cleaned_text'], \n",
    "                x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "            ), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Extracting the components of sentiment\n",
    "        sample_df['sentiment_overall'] = sample_df['sentiment_scores'].apply(lambda x: x['overall'])\n",
    "        sample_df['sentiment_ai_impact'] = sample_df['sentiment_scores'].apply(lambda x: x['ai_impact'])\n",
    "        sample_df['sentiment_industry'] = sample_df['sentiment_scores'].apply(lambda x: x['industry_context'])\n",
    "        \n",
    "        self._save_to_cache(sample_df, cache_file)\n",
    "        \n",
    "        return sample_df\n",
    "    \n",
    "def _analyze_contextual_sentiment(self, \n",
    "                                     text: str, \n",
    "                                     industry: Optional[str] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze sentiment with context awareness for AI impact\n",
    "        \n",
    "        Args:\n",
    "            text: Article text\n",
    "            industry: Specific industry to contextualize sentiment\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with different sentiment scores\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {'overall': 0, 'ai_impact': 0, 'industry_context': 0}\n",
    "        \n",
    "        # Base sentiment with TextBlob\n",
    "        base_sentiment = TextBlob(text).sentiment.polarity\n",
    "        \n",
    "        # AI impact sentiment using my custom dictionaries\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Occurrences and getting the weighted sentiment for domain-specific terms\n",
    "        positive_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.positive_terms.items()\n",
    "        )\n",
    "        \n",
    "        negative_sentiment = sum(\n",
    "            value * text_lower.count(term) \n",
    "            for term, value in self.negative_terms.items()\n",
    "        )\n",
    "        \n",
    "        # Domain-specific sentiment\n",
    "        ai_impact_mentions = sum(text_lower.count(term) for term in self.positive_terms) + \\\n",
    "                             sum(text_lower.count(term) for term in self.negative_terms)\n",
    "        \n",
    "        if ai_impact_mentions > 0:\n",
    "            ai_impact_sentiment = (positive_sentiment + negative_sentiment) / ai_impact_mentions\n",
    "        else:\n",
    "            ai_impact_sentiment = 0\n",
    "        \n",
    "        # Industry context sentiment\n",
    "        industry_context_sentiment = 0\n",
    "        if industry:\n",
    "            # Industry-specific terms\n",
    "            industry_terms = []\n",
    "            \n",
    "            if industry == 'healthcare':\n",
    "                industry_terms = ['patient', 'doctor', 'nurse', 'hospital', 'medical', 'diagnosis', 'treatment']\n",
    "            elif industry == 'finance':\n",
    "                industry_terms = ['bank', 'investment', 'trading', 'financial', 'insurance', 'loan']\n",
    "            elif industry == 'manufacturing':\n",
    "                industry_terms = ['factory', 'production', 'assembly', 'industrial', 'manufacturing']\n",
    "            elif industry == 'technology':\n",
    "                industry_terms = ['software', 'hardware', 'startup', 'tech', 'computing', 'digital']\n",
    "            elif industry == 'education':\n",
    "                industry_terms = ['student', 'teacher', 'school', 'learning', 'education', 'university']\n",
    "            \n",
    "            if industry_terms:\n",
    "                # Finding sentences containing industry terms\n",
    "                sentences = text.split('.')\n",
    "                industry_sentences = [s for s in sentences if any(term in s.lower() for term in industry_terms)]\n",
    "                \n",
    "                if industry_sentences:\n",
    "                    # Sentiment for industry-specific sentences\n",
    "                    industry_sentiment = np.mean([TextBlob(s).sentiment.polarity for s in industry_sentences])\n",
    "                    industry_context_sentiment = industry_sentiment\n",
    "        \n",
    "        # Sentiment is a weighted average of the base sentiment and AI impact sentiment\n",
    "        overall_sentiment = 0.4 * base_sentiment + 0.6 * ai_impact_sentiment\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_sentiment,\n",
    "            'ai_impact': ai_impact_sentiment,\n",
    "            'industry_context': industry_context_sentiment\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(self, \n",
    "                          df: pd.DataFrame,\n",
    "                          num_topics: int = 10,\n",
    "                          force_recompute: bool = False) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run topic modeling on articles\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with cleaned articles\n",
    "            num_topics: Number of topics to extract\n",
    "            force_recompute: Whether to force recomputation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (topic_model, document_topics)\n",
    "        \"\"\"\n",
    "        cache_file = f\"topic_model_{num_topics}.pkl\"\n",
    "        \n",
    "        # Results\n",
    "        if not force_recompute:\n",
    "            cached_data = self._load_from_cache(cache_file)\n",
    "            if cached_data is not None:\n",
    "                print(\"Loaded topic model from cache\")\n",
    "                return cached_data\n",
    "        \n",
    "        print(\"Running topic modeling...\")\n",
    "        \n",
    "        documents = df['cleaned_text'].tolist()\n",
    "        \n",
    "        # Embedding\n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Vectorizer with bigrams and trigrams\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words=\"english\", \n",
    "            ngram_range=(1, 3),\n",
    "            min_df=5,\n",
    "            max_df=0.7\n",
    "        )\n",
    "        \n",
    "        # BERTopic model\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            nr_topics=num_topics\n",
    "        )\n",
    "        \n",
    "        topics, probs = topic_model.fit_transform(documents)\n",
    "        \n",
    "        # Topic representations\n",
    "        topic_model.update_topics(documents, topics, n_gram_range=(1, 3))\n",
    "        \n",
    "        self._save_to_cache((topic_model, topics), cache_file)\n",
    "        \n",
    "        return topic_model, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(self, df: pd.DataFrame) -> Dict[str, plt.Figure]:\n",
    "        figures = {}\n",
    "        \n",
    "        # Industry Impact Heat Map\n",
    "        figures['industry_heatmap'] = self._create_industry_impact_heatmap(df)\n",
    "        \n",
    "        # Sentiment Timeline\n",
    "        figures['sentiment_timeline'] = self._create_sentiment_timeline(df)\n",
    "        \n",
    "        # Technology Adoption Timeline\n",
    "        figures['technology_timeline'] = self._create_technology_timeline(df)\n",
    "        \n",
    "        # AI organization Strategy Comparison\n",
    "        if 'top_organizations' in df.columns and 'ai_technologies' in df.columns:\n",
    "            figures['org_strategy'] = self._create_organization_ai_strategy_comparison(df)\n",
    "        \n",
    "        # Job Impact Network\n",
    "        figures['job_impact'] = self._create_tech_job_impact_network(df)\n",
    "        \n",
    "        # Topic Distribution\n",
    "        if hasattr(self, 'topics') and len(self.topics) == len(df):\n",
    "            figures['topic_timeline'] = self._plot_topics_over_time(df, self.topics)\n",
    "        \n",
    "        return figures\n",
    "    \n",
    "def _create_industry_impact_heatmap(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create a heatmap showing AI's impact on different industries based on sentiment\"\"\"\n",
    "        # Creating one row per industry\n",
    "        exploded_df = df.explode('detected_industries').dropna(subset=['detected_industries'])\n",
    "        \n",
    "        # Sentiment scores\n",
    "        if 'sentiment_ai_impact' in exploded_df.columns:\n",
    "            exploded_df['sentiment_score'] = exploded_df['sentiment_ai_impact']\n",
    "        else:\n",
    "            exploded_df['sentiment_score'] = exploded_df['sentiment_overall']\n",
    "        \n",
    "        # Articles per industry\n",
    "        industry_counts = exploded_df['detected_industries'].value_counts()\n",
    "        \n",
    "        # Top industries\n",
    "        top_industries = industry_counts.head(10).index.tolist()\n",
    "        \n",
    "        # Top industries\n",
    "        industry_df = exploded_df[exploded_df['detected_industries'].isin(top_industries)]\n",
    "        \n",
    "        # Grouping by industry and year\n",
    "        industry_df['year'] = industry_df['date'].dt.year\n",
    "        industry_year_sentiment = industry_df.groupby(['detected_industries', 'year'])['sentiment_score'].mean().reset_index()\n",
    "        \n",
    "        pivot_df = industry_year_sentiment.pivot(index='detected_industries', columns='year', values='sentiment_score')\n",
    "        \n",
    "        #  Heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(pivot_df, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
    "        plt.title('AI Impact Sentiment by Industry and Year')\n",
    "        plt.ylabel('Industry')\n",
    "        plt.xlabel('Year')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_sentiment_timeline(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create timeline of sentiment with key events\"\"\"\n",
    "        # Average sentiment by month\n",
    "        df['yearmonth'] = df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Sentiment scores\n",
    "        if 'sentiment_ai_impact' in df.columns:\n",
    "            sentiment_col = 'sentiment_ai_impact'\n",
    "        elif 'sentiment_overall' in df.columns:\n",
    "            sentiment_col = 'sentiment_overall'\n",
    "        else:\n",
    "            sentiment_col = 'sentiment' if 'sentiment' in df.columns else None\n",
    "        \n",
    "        if not sentiment_col:\n",
    "            print(\"No sentiment column found for timeline\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        # By month\n",
    "        monthly_sentiment = df.groupby('yearmonth')[sentiment_col].mean().reset_index()\n",
    "        \n",
    "        # Datetime format\n",
    "        monthly_sentiment['date'] = pd.to_datetime(monthly_sentiment['yearmonth'] + '-01')\n",
    "        monthly_sentiment = monthly_sentiment.sort_values('date')\n",
    "        \n",
    "        # Key AI events to annotate\n",
    "        key_events = [\n",
    "            {'date': '2023-11', 'event': 'GPT-4 Release', 'y_pos': 0.1},\n",
    "            {'date': '2024-03', 'event': 'AI Regulation Act', 'y_pos': -0.1},\n",
    "            {'date': '2024-06', 'event': 'Major AI Job Study', 'y_pos': 0.2},\n",
    "            {'date': '2024-09', 'event': 'New NLP Breakthrough', 'y_pos': -0.2}\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        \n",
    "        # Sentiment line\n",
    "        plt.plot(monthly_sentiment['date'], monthly_sentiment[sentiment_col], \n",
    "                 marker='o', linestyle='-', color='blue', alpha=0.7)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Event annotations\n",
    "        for event in key_events:\n",
    "            event_date = pd.to_datetime(event['date'] + '-01')\n",
    "            \n",
    "            if event_date < monthly_sentiment['date'].min() or event_date > monthly_sentiment['date'].max():\n",
    "                continue\n",
    "                \n",
    "            # Closest sentiment value\n",
    "            closest_idx = (monthly_sentiment['date'] - event_date).abs().idxmin()\n",
    "            event_sentiment = monthly_sentiment.loc[closest_idx, sentiment_col]\n",
    "            \n",
    "            plt.annotate(event['event'], \n",
    "                        xy=(event_date, event_sentiment),\n",
    "                        xytext=(event_date, event_sentiment + event['y_pos']),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                        fontsize=10)\n",
    "        \n",
    "        plt.title('AI Sentiment Timeline with Key Events')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sentiment Score (-1 to 1)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_technology_timeline(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create timeline showing the adoption/mention of AI technologies\"\"\"\n",
    "        if 'ai_technologies' not in df.columns:\n",
    "            print(\"No technology data found for timeline\")\n",
    "            return plt.figure()\n",
    "        \n",
    "\n",
    "        tech_rows = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            date = row['date']\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            \n",
    "            for tech_category, tech_items in techs.items():\n",
    "                if isinstance(tech_items, list):\n",
    "                    for tech in tech_items:\n",
    "                        tech_rows.append({\n",
    "                            'date': date,\n",
    "                            'tech_category': tech_category,\n",
    "                            'tech_item': tech,\n",
    "                            'count': 1\n",
    "                        })\n",
    "                else:\n",
    "                    tech_rows.append({\n",
    "                        'date': date,\n",
    "                        'tech_category': tech_category,\n",
    "                        'tech_item': str(tech_items),\n",
    "                        'count': 1\n",
    "                    })\n",
    "        \n",
    "        # Dataframe\n",
    "        if not tech_rows:\n",
    "            return plt.figure()\n",
    "            \n",
    "        tech_df = pd.DataFrame(tech_rows)\n",
    "        \n",
    "        # Grouping by month and technology category\n",
    "        tech_df['yearmonth'] = tech_df['date'].dt.strftime('%Y-%m')\n",
    "        monthly_tech = tech_df.groupby(['yearmonth', 'tech_category']).size().reset_index(name='count')\n",
    "        \n",
    "        # Date\n",
    "        monthly_tech['date'] = pd.to_datetime(monthly_tech['yearmonth'] + '-01')\n",
    "        monthly_tech = monthly_tech.sort_values('date')\n",
    "        \n",
    "        # Adoption over time\n",
    "        monthly_tech['cumulative'] = monthly_tech.groupby('tech_category')['count'].cumsum()\n",
    "        \n",
    "        pivot_df = monthly_tech.pivot(index='date', columns='tech_category', values='cumulative').fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        pivot_df.plot(kind='line', figsize=(14, 7))\n",
    "        plt.title('AI Technology Adoption Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Cumulative Mentions')\n",
    "        plt.legend(title='Technology', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n",
    "    \n",
    "def _create_organization_ai_strategy_comparison(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Compare different organizations' AI strategies based on sentiment and technology focus\"\"\"\n",
    "        # Ensure we have the necessary entity data\n",
    "        if 'top_organizations' not in df.columns or 'ai_technologies' not in df.columns:\n",
    "            print(\"Required columns missing for organization comparison\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        # Explode organizations\n",
    "        org_df = df.explode('top_organizations').dropna(subset=['top_organizations'])\n",
    "        \n",
    "        # Get top mentioned organizations\n",
    "        top_orgs = org_df['top_organizations'].value_counts().head(10).index.tolist()\n",
    "        \n",
    "        # Filter for top orgs\n",
    "        top_org_df = org_df[org_df['top_organizations'].isin(top_orgs)]\n",
    "        \n",
    "        # Create tech-org matrix\n",
    "        org_tech_data = []\n",
    "        \n",
    "        for _, row in top_org_df.iterrows():\n",
    "            org = row['top_organizations']\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            \n",
    "            if not techs:\n",
    "                continue\n",
    "                \n",
    "            # Get sentiment\n",
    "            sentiment = 0\n",
    "            if 'sentiment_overall' in row:\n",
    "                sentiment = row['sentiment_overall']\n",
    "            elif 'sentiment' in row:\n",
    "                sentiment = row['sentiment']\n",
    "            \n",
    "            # Add tech categories\n",
    "            for tech in techs:\n",
    "                org_tech_data.append({\n",
    "                    'organization': org,\n",
    "                    'technology': tech,\n",
    "                    'sentiment': sentiment,\n",
    "                    'count': 1\n",
    "                })\n",
    "        \n",
    "        if not org_tech_data:\n",
    "            return plt.figure()\n",
    "            \n",
    "        # Create dataframe\n",
    "        org_tech_df = pd.DataFrame(org_tech_data)\n",
    "        \n",
    "        # Group by org and tech\n",
    "        org_tech_summary = org_tech_df.groupby(['organization', 'technology']).agg(\n",
    "            sentiment=('sentiment', 'mean'),\n",
    "            count=('count', 'sum')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Get top technologies\n",
    "        top_techs = org_tech_df['technology'].value_counts().head(7).index.tolist()\n",
    "        \n",
    "        # Filter for top techs\n",
    "        plot_data = org_tech_summary[org_tech_summary['technology'].isin(top_techs)]\n",
    "        \n",
    "        # Create bubble chart\n",
    "        for tech in top_techs:\n",
    "            tech_data = plot_data[plot_data['technology'] == tech]\n",
    "            plt.scatter(tech_data['organization'], tech_data['sentiment'], \n",
    "                       s=tech_data['count'] * 50, alpha=0.7, label=tech)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.title('Organization AI Strategy Comparison')\n",
    "        plt.xlabel('Organization')\n",
    "        plt.ylabel('Sentiment Score (-1 to 1)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Technology Focus', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def _create_tech_job_impact_network(self, df: pd.DataFrame) -> plt.Figure:\n",
    "        \"\"\"Create network visualization showing how technologies impact different job roles\"\"\"\n",
    "        if 'ai_technologies' not in df.columns or 'detected_jobs' not in df.columns:\n",
    "            print(\"Required columns missing for job impact network\")\n",
    "            return plt.figure()\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Iterate through articles\n",
    "        for _, row in df.iterrows():\n",
    "            techs = row.get('ai_technologies', {})\n",
    "            jobs = row.get('detected_jobs', [])\n",
    "            \n",
    "            if not techs or not jobs:\n",
    "                continue\n",
    "            \n",
    "            # Get sentiment\n",
    "            sentiment = 0  # Default neutral\n",
    "            if 'sentiment_overall' in row:\n",
    "                sentiment = row['sentiment_overall']\n",
    "            elif 'sentiment' in row:\n",
    "                sentiment = row['sentiment']\n",
    "            \n",
    "            # Add edges between technologies and jobs\n",
    "            for tech_category in techs:\n",
    "                # Add tech node\n",
    "                if not G.has_node(tech_category):\n",
    "                    G.add_node(tech_category, type='technology')\n",
    "                \n",
    "                # Add job nodes and connect to tech\n",
    "                for job in jobs:\n",
    "                    if not G.has_node(job):\n",
    "                        G.add_node(job, type='job')\n",
    "                    \n",
    "                    # Add or update edge\n",
    "                    if G.has_edge(tech_category, job):\n",
    "                        # Update with new sentiment info\n",
    "                        current = G[tech_category][job]\n",
    "                        current['weight'] = (current['weight'] * current['count'] + sentiment) / (current['count'] + 1)\n",
    "                        current['count'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(tech_category, job, weight=sentiment, count=1)\n",
    "        \n",
    "        # If graph is empty, return empty figure\n",
    "        if len(G.edges()) == 0:\n",
    "            return plt.figure()\n",
    "        \n",
    "        # Filter graph to show only significant relationships (those with multiple mentions)\n",
    "        significant_edges = [(u, v) for u, v, d in G.edges(data=True) if d['count'] >= 2]\n",
    "        \n",
    "        # If no significant edges, use all edges\n",
    "        if not significant_edges:\n",
    "            significant_edges = G.edges()\n",
    "            \n",
    "        SG = G.edge_subgraph(significant_edges).copy()\n",
    "        \n",
    "        # Draw the network\n",
    "        pos = nx.spring_layout(SG, seed=42)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Draw nodes with different colors for tech vs jobs\n",
    "        tech_nodes = [n for n, d in SG.nodes(data=True) if d.get('type') == 'technology']\n",
    "        job_nodes = [n for n, d in SG.nodes(data=True) if d.get('type') == 'job']\n",
    "        \n",
    "        # Size nodes by degree\n",
    "        tech_sizes = [SG.degree(n) * 50 for n in tech_nodes]\n",
    "        job_sizes = [SG.degree(n) * 50 for n in job_nodes]\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(SG, pos, nodelist=tech_nodes, node_color='lightblue', \n",
    "                              node_size=tech_sizes, alpha=0.8)\n",
    "        nx.draw_networkx_nodes(SG, pos, nodelist=job_nodes, node_color='lightgreen',\n",
    "                              node_size=job_sizes, alpha=0.8)\n",
    "        \n",
    "        # Draw edges with colors based on sentiment\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        for u, v, d in SG.edges(data=True):\n",
    "            if d['weight'] > 0.2:\n",
    "                edge_colors.append('green')\n",
    "            elif d['weight'] < -0.2:\n",
    "                edge_colors.append('red')\n",
    "            else:\n",
    "                edge_colors.append('gray')\n",
    "            \n",
    "            # Edge width based on count\n",
    "            edge_widths.append(1 + d['count'] * 0.5)\n",
    "        \n",
    "        nx.draw_networkx_edges(SG, pos, width=edge_widths, edge_color=edge_colors, alpha=0.6)\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(SG, pos, font_size=10)\n",
    "        \n",
    "        plt.title('AI Technology Impact on Job Roles')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def _plot_topics_over_time(self, df: pd.DataFrame, topics: np.ndarray, top_n_topics: int = 5) -> plt.Figure:\n",
    "        \"\"\"Plot the prevalence of top topics over time\"\"\"\n",
    "        # Create a dataframe with topics and dates\n",
    "        topic_df = pd.DataFrame({'date': df['date'], 'topic': topics})\n",
    "        \n",
    "        # Get counts of each topic\n",
    "        topic_counts = Counter(topics)\n",
    "        top_topics = [topic for topic, count in topic_counts.most_common(top_n_topics) if topic != -1]\n",
    "        \n",
    "        # Filter for top topics and group by month\n",
    "        topic_df = topic_df[topic_df['topic'].isin(top_topics)]\n",
    "        topic_df['yearmonth'] = topic_df['date'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Count topics per month\n",
    "        topic_time = topic_df.groupby(['yearmonth', 'topic']).size().reset_index(name='count')\n",
    "        \n",
    "        # Convert yearmonth to datetime for better plotting\n",
    "        topic_time['date'] = pd.to_datetime(topic_time['yearmonth'] + '-01')\n",
    "        topic_time = topic_time.sort_values('date')\n",
    "        \n",
    "        # Pivot for plotting\n",
    "        pivot_df = topic_time.pivot(index='date', columns='topic', values='count').fillna(0)\n",
    "        \n",
    "        # Get topic labels if available\n",
    "        topic_labels = {}\n",
    "        if hasattr(self, 'topic_model'):\n",
    "            for topic in top_topics:\n",
    "                if topic != -1:  # Skip outlier topic\n",
    "                    # Get topic representation\n",
    "                    words = self.topic_model.get_topic(topic)\n",
    "                    if words:\n",
    "                        # Create label from top 3 words\n",
    "                        label = ', '.join([word for word, _ in words[:3]])\n",
    "                        topic_labels[topic] = f\"Topic {topic}: {label}\"\n",
    "                    else:\n",
    "                        topic_labels[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # If we have topic labels, rename columns\n",
    "        if topic_labels:\n",
    "            pivot_df = pivot_df.rename(columns=topic_labels)\n",
    "            \n",
    "        pivot_df.plot(kind='line', figsize=(12, 6))\n",
    "        plt.title('Top Topics Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Article Count')\n",
    "        plt.legend(title='Topic', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
